b'2018 Kaggle ML & DS Survey Challenge',"b'Explore the 2018 Kaggle ML & Data Science Survey for $28,000 in cash prizes'","b""# Overview\n\nWelcome to Kaggle's second annual Machine Learning and Data Science Survey \xe2\x80\x95 and our first-ever survey data challenge.\n\nThis year, [as last year][1], we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for one week in October, and after cleaning the data we finished with 23,859 responses, a 49% increase over last year!\n\nThere's a lot to explore here. The results include raw numbers about who is working with data, what\xe2\x80\x99s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset.\n\n\n---\n\n# Challenge\n\nThis year Kaggle is launching the first Data Science Survey Challenge, where we will be awarding a prize pool of $28,000 to kernel authors who tell a rich story about a subset of the data science and machine learning community.. \n\nIn our second year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This [survey data EDA][4] provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we\xe2\x80\x99re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.  \n\n**The challenge objective:** tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \xe2\x80\x9cstory\xe2\x80\x9d could be defined any number of ways, and that\xe2\x80\x99s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! \n\nSubmissions will be evaluated on the following: \n\n - Composition - Is there a clear narrative thread to the story that\xe2\x80\x99s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations. \n - Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.  \n - Documentation - Are your code,  and kernel, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible \n\nTo be valid, a submission must be contained in one kernel, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. \n\n\n---\n\n# Prizes\n\nThere will be 6 prizes for the best data storytelling submissions:  \n\n- 1st place:  $8,000 \n- 2nd place: $5,000 \n- 3rd place: $3,000\n- 4th place:  $2,000 \n- 5th place: $2,000 \n- 6th place: $2,000\n\nWinning submissions will also be published in Kaggle\xe2\x80\x99s blog [No Free Hunch][2], and shared in the Kaggle Newsletter (which goes out to nearly one million Kagglers!) \n\nWhile the challenge is running, Kaggle will also give a **Weekly Kernel Award of $1,500** to recognize excellent kernels that are public analyses of the survey. Weekly Kernel Awards will be announced every Friday between 11/9 and 11/30.  \n\n---\n# How to Participate \n\nTo make a submission, complete the [submission form][3]. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry. \n\nNo submission is necessary for the Weekly Kernels Awards. To be eligible, a kernel must be public and use the 2018 Data Science Survey as a data source. \n\n---\n\n# Timeline\n\nAll dates are 11:59PM UTC\n\n- Submission deadline: December 3rd\n\n- Winners announced: December 10th\n\n- Weekly Kernels Award prize winners announcements: November 9th, 16th, 23rd, and 30th\n\nAll kernels are evaluated after the deadline.\n\n---\n\n# Rules\n\nTo be eligible to win a prize in either of the above prize tracks, you must be:\n\n- a registered account holder at Kaggle.com;\n- the older of 18 years old or the age of majority in your jurisdiction of residence; and\n- not a resident of Crimea, Cuba, Iran, Syria, North Korea, or Sudan\n\nYour kernels will only be eligible to win if they have been made public on kaggle.com by the above deadline. All prizes are awarded at the discretion of Kaggle. Kaggle reserves the right to cancel or modify prize criteria.\n\nUnfortunately employees, interns, contractors, officers and directors of Kaggle Inc., and their parent companies, are not eligible to win any prizes.\n\n---\n\n# Survey Methodology\n\n - This survey received 23,859 usable respondents from 147 countries and\n   territories. If a country or territory received less than 50\n   respondents, we grouped them into a group named \xe2\x80\x9cOther\xe2\x80\x9d for\n   anonymity.\n   \n - We excluded respondents who were flagged by our survey system as\n   \xe2\x80\x9cSpam\xe2\x80\x9d.\n   \n - Most of our respondents were found primarily through Kaggle channels,\n   like our email list, discussion forums and social media channels.\n   \n - The survey was live from October 22nd to October 29th. We allowed\n   respondents to complete the survey at any time during that window. \n   The median response time for those who participated in the survey was\n   15-20 minutes. \n   \n - Not every question was shown to every respondent. You can learn more\n   about the different segments we used in the schema.csv file.\n   \n - To protect the respondents\xe2\x80\x99 identity, the answers to multiple choice\n   questions have been separated into a separate data file from the\n   open-ended responses. We do not provide a key to match up the\n   multiple choice and free form responses. Further, the free form\n   responses have been randomized column-wise such that the responses\n   that appear on the same row did not necessarily come from the same\n   survey-taker.\n\n\n\n__________________________________________________\n\n\n  [1]: https://www.kaggle.com/kaggle/kaggle-survey-2017\n  [2]: http://blog.kaggle.com/\n  [3]: https://www.kaggle.com/page/2018-data-science-survey-challenge-submission\n  [4]: https://www.kaggle.com/paultimothymooney/2018-kaggle-ml-and-ds-survey-eda""","b""['survey analysis', 'small', 'featured']"""
b'Data Science for Good: Center for Policing Equity',b'How do you measure justice?',"b'# Overview\n\n---\n\nThe Center for Policing Equity (CPE) is research scientists, race and equity experts, data virtuosos, and community trainers working together to build more fair and just systems. Data and science are our tools; law enforcement and communities are our partners. Our aim is to bridge the divide created by communication problems, suffering and generational mistrust, and forge a path towards public safety, community trust, and racial equity.\n\nPolice departments across the United States have joined our National Justice Database, the first and largest collection of standardized police behavioral data. In exchange for unprecedented access to their records (such as use of force incidents, vehicle stops, pedestrian stops, calls for service, and crime data), our scientists use advanced analytics to diagnose disparities in policing, shed light on police behavior, and provide actionable recommendations. Our highly-detailed custom reports help police departments improve public safety, restore trust, and do their work in a way that aligns with their own values.\n\n\n\n# Problem Statement\n\n---\n\nHow do you measure justice? And how do you solve the problem of racism in policing? We look for factors that drive racial disparities in policing by analyzing census and police department deployment data. The ultimate goal is to inform police agencies where they can make improvements by identifying deployment areas where racial disparities exist and are not explainable by crime rates and poverty levels.\n\nOur biggest challenge is automating the combination of police data, census-level data, and other socioeconomic factors. Shapefiles are unusual and messy -- which makes it difficult to, for instance, generate maps of police behavior with precinct boundary layers mixed with census layers. Police incident data are also very difficult to normalize and standardize across departments since there are no federal standards for data collection..\n\nMain Prize Track submissions will be judged based on the following general criteria:\n\n- **Performance** - How well does the solution combine shapefiles and census data? How much manual effort is needed? CPE will not be able to live-test every submission, so a strong entry will be able to automate using shape files with different projections and clearly articulate why it is effective at tackling the problem.\n\n- **Accuracy** - Does the solution provide reliable and accurate analysis? How well does it match census-level demographics to police deployment areas?\n\n- **Approachability** - The best solutions should use best coding practices and have useful comments. Plots and graphs should should be self-explanatory. CPE might use your work to explain to stakeholders where to take action,so the results of your solution should be developed for an audience of law enforcement professionals and public officials.\n\n\n\n# How to Participate \n\n---\n\n\n### Accept the Rules\nTo be considered a participant in the CPE Data Science for Good Event you must register and accept the rules.\n\nAccept the rules by filling out this form: [SIGNUP FORM](https://www.kaggle.com/data-science-for-good-cpe-signup)<br>\n(You need to be logged into your Kaggle account)<br>\n\n### Make Submissions\n\nMain Prize Track:\n\n* Be a registered participant by accepting the rules<br>\n* Make your kernel public<br>\n* Submit your kernel(s) by filling out this form [SUBMISSION FORM](https://www.kaggle.com/data-science-for-good-cpe-submission)<br>\n\nSecondary Prize Track:<br>\n\n* Be a registered participant by accepting the rules<br>\n* Make sure your kernel is public<br>\n* Note: No need to fill out the submission form for this prize track.\n\n\n\n\n# Prizes and Eligibility\n\n---\n\n### Main Prize Track ($15,000 total)\n\nCPE will award $15,000 in total prizes to five winning authors who submit public kernels that effectively tackle the objective. These kernels must be submitted for consideration by the deadline.\n\nPrizes:<br>\n\n- 1st place:  $5,000\n- 2nd place: $4,000\n- 3rd place: $3,000\n- 4th place: $1,500\n- 5th place: $1,500\n\n### Secondary Prize Track (swag)\n\nTo encourage collaboration through sharing of code and use of publicly available data, secondary prize awards will be based on popularity (upvotes). Winners will be the authors of the top five most upvoted kernels. \n\nPrizes are the winner\xe2\x80\x99s choice of:<br>\n\n- Kaggle No Free Hunch T-shirt\n- Kaggle Tier T-shirt\n- Kaggle Coffee Mug\n- Kaggle Water Bottle\n\n\n# Timeline\n\n---\n\n\nAll dates are 11:59PM UTC\n\n- Deadline for secondary prize submissions: **October 30th**\n\n- Deadline for main prize submissions: **December 4th**\n\n- Main prize winners announcement: **December 11th**\n\n\n\n# Rules\n\n---\n\nTo be eligible to win a prize in either of the above prize tracks, you must be:\n\n- a registered account holder at Kaggle.com;\n- the older of 18 years old or the age of majority in your jurisdiction of residence;\n- not a resident of Crimea, Cuba, Iran, Syria, North Korea, or Sudan; and\n- not a person or representative of an entity under U.S. export controls or sanctions.\n\nYour kernels will only be eligible to win if they have been made public on kaggle.com by the above deadline. All prizes are awarded at the discretion of CPE, and CPE reserves the right to cancel or modify prize criteria.\n\nUnfortunately employees, interns, contractors, officers and directors of Kaggle Inc., and their parent companies, are not eligible to win any prizes.\n\n'","b""['demographics', 'geospatial analysis', 'government agencies', 'communities', 'medium', 'featured']"""
b'Students Performance in Exams',b'Marks secured by the students in various subjects',"b'### Context\n\nMarks secured by the students\n\n\n### Content\n\nThis data set consists of the marks secured by the students in various subjects. \n\n\n### Acknowledgements\n\nhttp://roycekimmons.com/tools/generated_data/exams\n\n\n### Inspiration\n\nTo understand the influence of the parents background, test preparation etc on students performance'","b""['data visualization', 'eda', 'small', 'featured']"""
b'Black Friday',b'A study of sales trough consumer behaviours',"b'## Description\n\nThe dataset here is a sample of the transactions made in a retail store. The store wants to know better the customer purchase behaviour against different products. Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.\n\nClassification problem can also be settled in this dataset since several variables are categorical, and some other approaches could be ""Predicting the age of the consumer"" or even ""Predict the category of goods bought"". \nThis dataset is also particularly convenient for clustering and maybe find different clusters of consumers within it.\n\n### Acknowledgements\n\nThe dataset comes from a competition hosted by Analytics Vidhya.'","b""['business', 'regression analysis', 'small', 'featured']"""
b'WHO Suicide Statistics',"b'Basic historical (1979-2016) data by country, year and demographic groups'","b""### Context and inspiration\n\nThis dataset was necessary for me to write my [Suicide in the Twenty-First Century][1] piece. \n\nLooking at the data, you can observe long-time trends and differences between countries, as well as within countries across a few demographic groups - in both cases you will see that these differences may be very large.\n\n\n### Content\n\nBasic aggregate numbers covering 1979-2016, by country, year, age groups and sex. There is only one file, with only a few columns. I made this file using the [WHO Mortality Database online tool][2].\n\n\n### Acknowledgements and license\n\nI prepared and uploaded the data file without WHO's knowledge, let alone approval or endorsement. Yet, the data itself belongs to WHO, so please see the [WHO copyrights, permission and licensing][3] pages if you want to use it.\n\nPhoto by [Daniela Rocha][4] on Unsplash.\n\n\n  [1]: https://www.kaggle.com/szamil/suicide-in-the-twenty-first-century/\n  [2]: http://apps.who.int/healthinfo/statistics/mortality/whodpms/\n  [3]: http://www.who.int/about/copyright/en/\n  [4]: https://unsplash.com/@danirocha16""","b""['demographics', 'sociology', 'social sciences', 'mortality', 'small', 'featured']"""
b'Los Angeles International Airport Data',b'From Los Angeles Open Data',"b""### Content  \n\nMore details about each file are in the individual file descriptions.  \n\n### Context  \n\nThis is a dataset hosted by the city of Los Angeles. The organization has an open data platform found [here](https://data.lacity.org) and they update their information according the amount of data that is brought in. Explore Los Angeles's Data using Kaggle and all of the data sources available through the city of Los Angeles [organization page](https://www.kaggle.com/cityofLA)!  \n\n* Update Frequency: This dataset is updated daily.\n\n### Acknowledgements\n\nThis dataset is maintained using Socrata's API and Kaggle's API. [Socrata](https://socrata.com/) has assisted countless organizations with hosting their open data and has been an integral part of the process of bringing more data to the public.  \n\nThis dataset is distributed under the following licenses: Creative Commons Attribution 4.0 International""","b""['socrata', 'small', 'featured']"""
b'Google Play Store Apps',b'Web scraped data of 10k Play Store apps for analysing the Android market.',"b'### Context\n\nWhile many public datasets (on Kaggle and the like) provide Apple App Store data, there are not many counterpart datasets available for Google Play Store apps anywhere on the web. On digging deeper, I found out that iTunes App Store page deploys a nicely indexed appendix-like structure to allow for simple and easy web scraping. On the other hand, Google Play Store uses sophisticated modern-day techniques (like dynamic page load) using JQuery making scraping more challenging.\n\n\n### Content\n\nEach app (row) has values for catergory, rating, size, and more.\n\n\n### Acknowledgements\n\nThis information is scraped from the Google Play Store. This app information would not be available without it.\n\n\n### Inspiration\n\nThe Play Store apps data has enormous potential to drive app-making businesses to success. Actionable insights can be drawn for developers to work on and capture the Android market!'","b""['internet', 'video games', 'computer science', 'mobile web', 'small', 'featured']"""
b'Wikipedia Movie Plots',"b'Plot descriptions for ~35,000 movies'","b'### Context\n\nPlot summary descriptions scraped from Wikipedia\n\n### Content\n\nThe dataset contains descriptions of 34,886 movies from around the world.   Column descriptions are listed below:\n\n* *Release Year* - Year in which the movie was released\n* *Title* - Movie title\n* *Origin/Ethnicity* - Origin of movie (i.e. American, Bollywood, Tamil, etc.)\n* *Director* - Director(s)\n* *Plot* - Main actor and actresses\n* *Genre* - Movie Genre(s)\n* *Wiki Page* - URL of the Wikipedia page from which the plot description was scraped\n* *Plot* - Long form description of movie plot (WARNING: May contain spoilers!!!)\n\n### Inspiration\n\n**Content-Based Movie Recommender:**\nRecommend movies with plots similar to those that a user has rated highly.\n\n**Movie Plot Generator:**\nGenerate a movie plot description based on seed input, such as director and genre\n\n**Information Retrieval:** \nReturn a movie title based on an input plot description\n\n**Text Classification:**\nPredict movie genre based on plot description\n\n### Acknowledgements\nThis data was scraped from Wikipedia'","b""['nlp', 'text data', 'recommender systems', 'film', 'medium', 'featured']"""
b'Historical Plane Crashes',b'Data from http://www.planecrashinfo.com',"b'### Context\nThere are some big plane crashes recently. I want to know more about the crashes. \nFor very first step, I need to collect data from somewhere, then I found http://www.planecrashinfo.com/database.htm\nYou guys can pull new data from [planecrashinfo.com][1] by using [https://github.com/hocnx/planecrashinfo_scraping][2]\n\n### Content\nData format:\nFormat\n\n    date:\t Date of accident,  in the format - January 01, 2001\n    time:\t Local time, in 24 hr. format unless otherwise specified\n    location: location information\n    Airline/Op:\t Airline or operator of the aircraft\n    flight_no:\t Flight number assigned by the aircraft operator\n    route:\t Complete or partial route flown prior to the accident\n    ac_type:\t Aircraft type\n    registration:\t ICAO registration of the aircraft\n    cn_ln:\t Construction or serial number / Line or fuselage number\n    aboard:\t Total aboard (passengers / crew)\n    fatalities:\t Total fatalities aboard (passengers / crew)\n    ground:\t Total killed on the ground\n    summary:\t Brief description of the accident and cause if known\n\n\n### Acknowledgements\nMany thanks to http://www.planecrashinfo.com\n\n### Inspiration\nI want to know the trending of number plane crash and the damage volume each year\n\n  [1]: http://www.planecrashinfo.com/database.htm\n  [2]: https://github.com/hocnx/planecrashinfo_scraping'","b""['small', 'featured']"""
b'Art Images: Drawing/Painting/Sculptures/Engravings',b'Dataset with about 9000 images containing 5 types of arts',"b'### Context\n\nDataset for classifying different styles of art. Main categories have been taken [here][1]\n\n### Content\n\n5 kinds of data downloaded from google images, yandex images and [this site][1]: \n\n 1. Drawings and watercolours\n 2. Works of painting\n 3. Sculpture\n 4. Graphic Art\n 5. Iconography (old Russian art)\n\nData is separated on training and validation sets\n\n\n  [1]: http://rusmuseumvrm.ru/collections/index.php?lang=en'","b""['image data', 'arts and entertainment', 'drawing', 'painting', 'sculpture', 'medium', 'featured']"""
b'English Wikipedia Articles 2017-08-20 SQLite',b'A dump of Wikipedia from 2017-08-20 converted to a SQLite database.',"b""### Context\n\nThis dataset was originally intended for the Data Science Nashville November 2018 meetup: [Introduction to Gensim][1]. I wanted to provide a large text corpus in a format often seen in industry, so I pulled the english Wikipedia dump from 2017-08-20, extracted the text using Gensim's excellent [segment_wiki][2] script, and finally wrote some custom code to populate a SQLite database.\n\nThe dataset encompasses nearly 5 million articles, with more than 23 million individual sections. Only article text is included, all links have been stripped and no metadata (e.g., behind the scene discussion or version history) is included. Even then, I just barely met the file size limit, coming in at just below 20 GB.\n\n### Content\n\nI wanted to keep things simple, so everything is in a single table: **articles**. There is an index on article_id.\n\n - **article_id**: Int, identifier for each unique title\n - **article_title**: Str, article titles\n - **section_title**: Str, subsection title from each article\n - **section_text**: Str, text from each subsection\n\n### Acknowledgements\n\nAs per The Wikimedia Foundation's [requirements][3], this dataset is provided under the Creative Commons Attribution-ShareAlike 3.0 Unported License. Permission is granted to copy, distribute, and/or modify Wikipedia's text under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unported License and, unless otherwise noted, the GNU Free Documentation License. unversioned, with no invariant sections, front-cover texts, or back-cover texts.\n\nThe banner image is provided by [Lysander Yuen][4] on [Unsplash][5].\n\n\n  [1]: https://www.meetup.com/Data-Science-Nashville/events/256605771/\n  [2]: https://radimrehurek.com/gensim/scripts/segment_wiki.html\n  [3]: https://en.wikipedia.org/wiki/Wikipedia:Database_download\n  [4]: https://unsplash.com/@_lysander_yuen\n  [5]: https://unsplash.com/""","b""['internet', 'text data', 'education', 'databases', 'large', 'featured']"""
b'National Footprint Accounts 2018',b'The Ecological Footprint of 196 Countries',"b'# National Footprint Accounts 2018 #\nThe National Footprint Accounts (NFA) are an annual production from Global Footprint Network (www.footprintnetwork.org). Each year, we combine and synthesize over 30 datasets to calculate the Ecological Footprint and biocapacity of countries across the world in over 50 years. \n\nThe goal of this undertaking is to produce accounts of how much area is required to provide the ecological services (resource regeneration and waste assimilation) consumed by humanity (""Ecological Footprint""), and how much biologically productive area exists to provide these ecological services (""biocapacity"") in each year. With both values in hand, we can assess the overall sustainability of countries around the world and better understand the collective need for humanity to reduce its impact on nature.\n\nThis data underpins both our [Overshoot Day][1] campaign and our [Personal Footprint Calculator][2], both of which you should check out! All of our data is also available on our [Data Platform][3].\n\n##Inspiration##\nAt Global Footprint Network we spend a lot of time calculating Ecological Footprint, and not as much time investigating and researching the trends we see. We are looking for cool stories, trends, interpretations, or visualizations that are meaningful to you or that you think would be meaningful to others. What is your country\'s footprint and biocapacity - and which is larger? Can you find out why? What about different regions or economic groups? \n\nBesides descriptive analytics, we are also driven to understand more about the causality of footprint values. Can you identify the effect of sustainable (or unsustainable) policies? Is there a good way to predict future values of footprint? Are there any external indicators closely tied to values of footprint?\n\nThere are many ways to slice the data, and we hope you have fun finding your own approach!\n\nSplash Photo by Alexey Topolyanskiy on Unsplash\n\n\n  [1]: https://www.overshootday.org/\n  [2]: https://www.footprintcalculator.org/\n  [3]: http://data.footprintnetwork.org/#/'","b""['economics', 'world', 'countries', 'environment', 'natural resources', 'small', 'featured']"""
b'Hourly Energy Consumption',b'Over 10 years of hourly energy consumption data from PJM in Megawatts',"b""### PJM Hourly Energy Consumption Data\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n\nThe hourly power consumption data comes from PJM's website and are in megawatts (MW).\n\nThe regions have changed over the years so data may only appear for certain dates per region.\n\n![Energy Plot][1]\n\n## Ideas of what you could do with this dataset:\n- Split the last year into a test set- can you build a model to predict energy consumption?\n- Find trends in energy consumption around hours of the day, holidays, or long term trends?\n- Understand how daily trends change depending of the time of year. Summer trends are very different than winter trends.\n\n![PJM Regions][2]\n\n\n  [1]: https://s15.postimg.cc/8rdtgokpn/download.png\n  [2]: https://www.theenergytimes.com/sites/theenergytimes.com/files/styles/article_featured_retina/public/pjm-image.jpg.crop_display.jpg?itok=XLQYO4j-""","b""['business', 'time series', 'united states', 'energy', 'research', 'medium', 'featured']"""
b'Happy House Dataset',b'Detect whether a person is smiling or not!',"b""### Context\n\nThis dataset was uploaded as because I didn't came across no quickly available sources for the happy house problem.\nSo, I took this dataset from Deep Learning Specialization - Course 4 - Convolutional Neural Networks, to give easy access to everyone on Kaggle to play around with it.\n\n\n### Content\n\nContains 2 *.h5 files\nOne is the train dataset\nOne is the test dataset\n\nHere's some boilerplate code to get you started with loading the dataset!\nNote: The code is borrowed from the Deep Learning Specialization Assignment which introduces this problem.\n\n    def load_dataset(path_to_train, path_to_test):\n        train_dataset = h5py.File(path_to_train)\n        train_x = np.array(train_dataset['train_set_x'][:])\n        train_y = np.array(train_dataset['train_set_y'][:])\n\n        test_dataset = h5py.File(path_to_test)\n        test_x = np.array(test_dataset['test_set_x'][:])\n        test_y = np.array(test_dataset['test_set_y'][:])\n\n        # y reshaped\n        train_y = train_y.reshape((1, train_x.shape[0]))\n        test_y = test_y.reshape((1, test_y.shape[0]))\n\n        return train_x, train_y, test_x, test_y\n\n\n### Acknowledgements\n\nDeep Learning Specialization - Andrew Ng and the whole deeplearning.ai team.\nImage taken from Pexels: https://www.pexels.com/photo/cute-family-picture-160994/""","b""['deep learning', 'image data', 'small', 'featured']"""
"b""100K Coursera's Course Reviews Dataset""",b'100K+ Scraped Course Reviews from the Coursera Website (As of May 2017)',"b'### Context\nThis dataset was used for my undergraduate research. The main problem in this dataset is its imbalanced nature.\n\n### Content\nI scraped the website of Coursera and pre-labelled the dataset depending on their rating. For a 5-star rating, the review was labelled as Very Positive, Positive for 4-star, Neutral for 3-star, Negative for 2-star, and Very Negative for 1-star. There are 2 files, **reviews.tsv** and **reviews_by_course.tsv**. The **reviews.tsv** file has no grouping, just the course reviews and their corresponding label. For the **reviews_by_course.tsv**, they are grouped by the CourseId column.\n\n### reviews.tsv\n**Id** - The unique identifier for a review.\n\n**Review** - The actual course review.\n\n**Label** - The rating of the course review.\n\n### reviews_by_course.tsv\n**CourseId** - The course tag. This is in the URL of the course in the Coursera website. For example, in this URL, [**machine-learning** would be the course tag][1].\n\n**Review** - A review in a specific course.\n\n**Label** - The rating of the course review.\n\n  [1]: https://www.coursera.org/learn/machine-learning'","b""['medium', 'featured']"""
b'Amazon Alexa Reviews ',"b'A list of 3150 Amazon customers reviews for Alexa Echo, Firestick, Echo Dot etc.'","b""**About the Data**\n\nThis dataset consists of a nearly 3000 Amazon customer reviews (input text), star ratings, date of review, variant  and feedback of  various amazon Alexa products like Alexa Echo, Echo dots, Alexa Firesticks etc. for learning how to train Machine for sentiment analysis.\n\n\n### What you can do with this Data ?\n\nYou can use this data to analyze Amazon\xe2\x80\x99s Alexa product ; discover insights into consumer reviews and assist with machine learning models.You can also train your machine models for sentiment analysis and analyze customer reviews how many positive reviews ? and how many negative reviews ?\n\n\n\n### Source\n\nExtracted from Amazon's website \n\n### Inspiration\n\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?""","b""['beginner', 'deep learning', 'nlp', 'naive bayes', 'customer value', 'small', 'featured']"""
b'Sarcasm on Reddit',b'1.3 million labelled comments from Reddit',"b'### Context \nThis dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The dataset was generated by scraping comments from Reddit (not by me :)) containing the `\\s` ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content.\n\n### Content\nData has balanced and imbalanced (i.e true distribution) versions. (True ratio is about 1:100). The\ncorpus has 1.3 million sarcastic statements, along with what they responded to as well as many non-sarcastic comments from the same source.\n\nLabelled comments are in the `train-balanced-sarcasm.csv` file.\n\n### Acknowledgements\n\nThe data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article ""[A Large Self-Annotated Corpus for Sarcasm](https://arxiv.org/abs/1704.05579)"". The data is hosted [here](http://nlp.cs.princeton.edu/SARC/0.0/).\n\nCitation:\n\n    @unpublished{SARC,\n      authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},\n      title={A Large Self-Annotated Corpus for Sarcasm},\n      url={https://arxiv.org/abs/1704.05579},\n      year=2017\n    }\n\n[Annotation of files in the original dataset: readme.txt][1].\n\n### Inspiration\n\n - Predicting sarcasm and relevant NLP features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, ""Internet Slang"" and specific phrases). \n - Sarcasm vs Sentiment\n - Unusual linguistic features such as caps, italics, or elongated words. e.g., ""Yeahhh, I\'m sure THAT is the right answer"".\n - Topics that people tend to react to sarcastically\n\n  [1]: http://nlp.cs.princeton.edu/SARC/0.0/readme.txt'","b""['internet', 'reddit', 'humor', 'medium', 'featured']"""
b'Marketing Funnel by Olist',"b'8k leads, closed deals and connection to 100k orders'","b'# Marketing Funnel by Olist\n\nWelcome! This is a marketing funnel dataset from sellers that filled-in requests of contact to sell their products on [Olist Store](http://www.olist.com). The dataset has information of 8k Marketing Qualified Leads (MQLs) that requested contact between Jun. 1st 2017 and Jun 1st 2018. They were randomly sampled from the total of MQLs. \n\nIts features allows viewing a sales process from multiple dimensions: lead category, catalog size, behaviour profile, etc. \n\nThis is real data, it has been anonymized and sampled from the original dataset.\n\n## Joining with Brazilian E-Commerce Public Dataset by Olist\nThis dataset can also be linked to the [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/olistbr/brazilian-ecommerce/home) using ```seller_id```. There you will find information of 100k orders, price, payment, freight performance, customer location, product attributes and finally reviews written by customers.\n\n**Instructions on joining are available on this [Kernel](https://www.kaggle.com/andresionek/joining-marketing-funnel-with-brazilian-e-commerce).**\n\n## Context\nThis dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on our website: [www.olist.com](https://www.olist.com)\n\n**A seller join Olist through a marketing and sales funnel that was made public at this dataset. Description of steps:**\n\n1. Sign-up at a landing page.\n1. Get contacted by a Sales development Representative (SDR), confirm some information and schedule a consultancy.\n1. Consultancy is made by a Sales Representative (SR). The SR may close the deal (lead sing up) or lose the deal (led leaves without sign in) \n1. Lead becomes a seller and starts building his catalog on Olist. \n1. His products are published on marketplaces and ready to sell!\n\n### Attention\n1. A seller MQL might come from multiple sources (he might subscribe on two different landing pages, for instance).\n\n### Examples of  Landing Pages\n![Example of a landing page](https://i.imgur.com/jKZTP5e.png)\n![](https://i.imgur.com/mAljYcq.png)\n\n## Data Schema\nThe data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:\n![](https://i.imgur.com/Jory0O3.png)\n\n## Inspiration\nHere are some inspiration for possible outcomes from this dataset.\n\n**Customer Lifetime Value:** <br>\nHow much a customer will bring in future revenue?\n\n**SR/SDR Optimization:**<br> \nWhich SR or SDR should talk with each kind of lead?\n\n**Closing Prediction:**<br> \nWhich deals will be closed?\n\n**EDA:**<br> \nJust Have Fun!\n\n## Acknowledgements\nThanks to Olist for releasing this dataset.'","b""['internet', 'marketing analytics', 'marketing', 'customer value', 'small', 'featured']"""
b'UN General Debates',b'Transcriptions of general debates at the UN from 1970 to 2016',"b'### Context: \n\nEvery year since 1947, representatives of UN member states gather at the annual sessions of the United Nations General Assembly. The centrepiece of each session is the General Debate. This is a forum at which leaders and other senior officials deliver statements that present their government\xe2\x80\x99s perspective on the major issues in world politics. These statements are akin to the annual legislative state-of-the-union addresses in domestic politics. This dataset, the UN General Debate Corpus (UNGDC), includes the corpus of texts of General Debate statements from 1970 (Session 25) to 2016 (Session 71).\n\n###Content: \n\nThis dataset includes the text of each country\xe2\x80\x99s statement from the general debate, separated by country, session and year and tagged for each. The text was scanned from PDFs of transcripts of the UN general sessions. As a result, the original scans included  page numbers in the text from OCR (Optical character recognition) scans, which have been removed. This dataset only includes English.\n\n### Acknowledgements: \n\nThis dataset was prepared by  Alexander Baturo, Niheer Dasandi, and Slava Mikhaylov, and is presented in the paper ""Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus"" Research & Politics, 2017.\n\n### Inspiration: \n\nThis dataset includes over forty years of data from different countries, which allows for the exploration of differences between countries and over time. This allows you to ask both country-specific and longitudinal questions. Some questions that might be interesting:\n\n* How has the sentiment of each country\xe2\x80\x99s general debate changed over time? \n* What topics have been more or less popular over time and by region? \n* Can you build a classifier which identifies which country a given text is from? \n* Are there lexical or syntactic changes over time or differences between region? \n* How does the latitude of a country affect lexical complexity?'","b""['linguistics', 'international relations', 'medium', 'featured']"""
b'Pollster Congressional Districts',b'Explore Pollster Political Open Data',"b""### Content  \n\nMore details about each file are in the individual file descriptions.  \n\n### Context  \n\nThis is a dataset hosted by the Huffington Post through HuffPost Pollster. The organization has poll/survey data available [here](https://elections.huffingtonpost.com/pollster) from various surveyors. Explore HuffPost Pollster data using Kaggle and all of the data sources available through the Huffington Post [organization page](https://www.kaggle.com/huffingtonpost)!  \n\n* Update Frequency: This dataset is updated Weekly.\n\n### Acknowledgements\n\nThis dataset is maintained using HuffPost Pollster's [API](https://app.swaggerhub.com/apis/huffpostdata/pollster-api/2.0.0) and Kaggle's API.""","b""['politics', 'small', 'featured']"""
