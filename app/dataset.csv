b'Names Corpus',b'5001 female names and 2943 male names',"b'### Context\n\nThis corpus contains 5001 female names and 2943 male names, sorted alphabetically, one per line created by Mark Kantrowitz and redistributed in NLTK. \n\nThe `names.zip` file includes \n \n  - README: The readme file.\n  - female.txt:  A line-delimited list of words.\n  - male.txt: A line-delimited list of words.\n\n### License/Usage\n\n    Names Corpus, Version 1.3 (1994-03-29)\n    Copyright (C) 1991 Mark Kantrowitz\n    Additions by Bill Ross\n    \n    This corpus contains 5001 female names and 2943 male names, sorted\n    alphabetically, one per line.\n    \n    You may use the lists of names for any purpose, so long as credit is\n    given in any published work. You may also redistribute the list if you\n    provide the recipients with a copy of this README file. The lists are\n    not in the public domain (I retain the copyright on the lists) but are\n    freely redistributable.  If you have any additions to the lists of\n    names, I would appreciate receiving them.\n    \n    Mark Kantrowitz <mkant+@cs.cmu.edu>\n    http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n\n\n### Inspiration\n\nThis corpus is used for the [text classification chapter in the NLTK book](http://www.nltk.org/book/ch06.html).'","b""['linguistics', 'small', 'featured']""",https://www.kaggle.com/nltkdata/names
b'Barcelona Unemployment',b'Barcelona registered unemployment percentages by hood and month',"b'### Context\n\nThis dataset represents the % of registered unemployment in the city of Barcelona (Spain) from year 2012 till 2016.\n\nRegistered unemployment corresponds to the job demands pending cover by the last day of each month, excluding employees who want to change jobs, the ones that do not have readily available or incompatible situation, the ones that are asking for a specific occupation and the temporary agricultural beneficiaries special unemployment benefit. \n\n\n### Content\n\nAll files in this dataset have the same format. Every row represents a hood from the city.\n\n 1. District number\n 2. Hood name\n 3. Number of citizens from this hood with ages between 16 and 64 (legal ages for having a job)\n 4. 12 columns (one per month), % of unemployment\n\nIn Barcelona we have hoods and districts. Every hood belongs to a district. A district is formed by several hoods.\n\n\n### Acknowledgements\n\nThis data can be found in ""[Open Data BCN - Barcelona\'s City Hall Open Data Service][1]"", which is the owner of the CSV files.\n\n\n### Inspiration\n\nA few weeks ago I needed this datasets for testing purposes.\n\nI have uploaded this information here, because, in my honest opinion, ""data"" and ""research"" should be shared with everybody. Enjoy!\n\n\n  [1]: http://opendata-ajuntament.barcelona.cat/en/'","b""['finance', 'business', 'small', 'featured']""",https://www.kaggle.com/marcvelmer/barcelona-unemployment
b'Crop Nutrient Database',b'USDA data about crop nutrients in the U.S.',"b""### Context\n\nThe PLANTS Database provides standardized information about the vascular plants, mosses, liverworts, hornworts, and lichens of the U.S. and its territories. It includes names, plant symbols, checklists, distributional data, species abstracts, characteristics, images, plant links, references, and crop information, and automated tools.\n\nThis particular dataset is the Crop Nutrient Database.\n\n### Content\n \nThese are the fields included in the dataset. I'll be honest, I have no idea what some of them mean:\n\n- Crop\n- ScientificName\n- Symbol\n- NuContAvailable\n- PlantPartHarvested\n- CropCategory\n- YieldUnit\n- AvYieldUnitWeight(lb)\n- AvMoisture%\n- AvN%(dry)\n- AvP%(dry)\n- AvK%(dry)\n- YieldUnitWeight(lb)_set\n- YieldUnitWeight(lb)_Bau\n- YieldUnitWeight(lb)_Joh\n- YieldUnitWeight(lb)_Roberts\n- YieldUnitWeight(lb)_WEEP\n- YieldUnitWeight(lb)_Men\n- YieldUnitWeight(lb)_Guy\n- YieldUnitWeight(lb)_Mc\n- YieldUnitWeight(lb)_Mah\n- YieldUnitWeight(lb)_Sha\n- YieldUnitWeight(lb)_Sch\n- YieldUnitWeight(lb)_Atu\n- YieldUnitWeight(lb)_Zim\n- YieldUnitWeight(lb)_Scu\n- YieldUnitWeight(lb)_John\n- YieldUnitWeight(lb)_Arc\n- DryMatter%_M-FF\n- DryMatter%_NAS\n- DryMatter%_F&L\n- DryMatter%_F&N\n- DryMatter%_Alb\n- DryMatter%_Est1\n- DryMatter%_Est2\n- DryMatter%_Est3\n- DryMatter%_Est4\n- DryMatter%_Est5\n- DryMatter%_Est6\n- DryMatter%_M&R\n- DryMatter%_M&L\n- DryMatter%_Sun\n- DryMatter%_Gro\n- DryMatter%_AgH8-9\n- DryMatter%_AgH8-12\n- DryMatter%_B788\n- AvDryMatter%\n- N%(dry)_NAS\n- N%(dry)_F&L\n- N%(dry)_F&N\n- N%(dry)_Swa\n- N%(dry)_Chapko\n- N%(dry)_Hill\n- N%(dry)_Bru\n- N%(dry)_AgH8-9\n- N%(dry)_AgH8-12\n- N%(dry)_B788\n- N%(dry)_M&L\n- N%(dry)_M-FF\n- N%(dry)_M&R\n- N%(dry)_Foster\n- N%(dry)_Rob1\n- N%(dry)_Rob2\n- N%(dry)_Coa\n- N%(dry)_And\n- N%(dry)_Gol1\n- N%(dry)_Gol2\n- N%(dry)_Wol\n- N%(dry)_Pete\n- N%(dry)_Col\n- N%(dry)_Alb\n- N%(dry)_Arc\n- N%(dry)_Bis\n- N%(dry)_Gar\n- N%(dry)_Heg\n- N%(dry)_Flo\n- N%(dry)_Feil\n- N%(dry)_Bre\n- N%(dry)_Burns\n- N%(dry)_Coc\n- P%(dry)_M-FF\n- P%(dry)_NAS\n- P%(dry)_F&L\n- P%(dry)_F&N\n- P%(dry)_AgH8-9\n- P%(dry)_AgH8-12\n- P%(dry)_B788\n- P%(dry)_M&L\n- P%(dry)_L&V\n- P%(dry)_Foster\n- P%(dry)_Rob1\n- P%(dry)_Rob2\n- P%(dry)_Coa\n- P%(dry)_And\n- P%(dry)_Gol1\n- P%(dry)_Gol2\n- P%(dry)_Sims\n- P%(dry)_Wol\n- P%(dry)_Pete\n- P%(dry)_Col\n- P%(dry)_Alb\n- P%(dry)_Arc\n- P%(dry)_Swa\n- P%(dry)_Rei\n- K%(dry)_M-FF\n- K%(dry)_NAS\n- K%(dry)_F&L\n- K%(dry)_F&N\n- K%(dry)_AgH8-9\n- K%(dry)_AgH8-12\n- K%(dry)_B788\n- K%(dry)_Foster\n- K%(dry)_Rob1\n- K%(dry)_Rob2\n- K%(dry)_Coa\n- K%(dry)_And\n- K%(dry)_Gol1\n- K%(dry)_Gol2\n- K%(dry)_Sims\n- K%(dry)_Wol\n- K%(dry)_Pete\n- K%(dry)_Col\n- K%(dry)_Alb\n- K%(dry)_Arc\n- K%(dry)_Swa\n- K%(dry)_Rei\n- Moisture%_M&R\n- Moisture%_M&L\n- Moisture%_Sun\n- Moisture%_Gro\n- gWater/100g_AgH8-9\n- gWater/100g_AgH8-12\n- gWater/100g_B788\n- Protein%(dry)_NAS\n- Protein%(dry)_F&L\n- Protein%(dry)_F&N\n- Protein%(dry)_Swa\n- Protein%(dry)_Chapko\n- Protein%(dry)_Hill\n- Protein%(dry)_Bru\n- Protein%(dry)_Bis\n- Protein%(dry)_Gar\n- Protein%(dry)_Heg\n- Protein%(dry)_Flo\n- Protein%(dry)_Feil\n- Protein%(dry)_Bre\n- Protein%(dry)_Burns\n- gProtein/100g(wet)_AgH8-9\n- gProtein/100g(wet)_AgH8-12\n- gProtein/100g(wet)_B788\n- Protein%(wet)_M&L\n- N%(wet)_M-FF\n- P%(wet)_M-FF\n- gP/100g(wet)_AgH8-9\n- gP/100g(wet)_AgH8-12\n- gP/100g(wet)_B788\n- P%(wet)_M&L\n- K%(wet)_M-FF\n- gK/100g(wet)_AgH8-9\n- gK/100g(wet)_AgH8-12\n- gK/100g(wet)_B788\n\n""","b""['food and drink', 'united states', 'health', 'plants', 'science and culture', 'small', 'featured']""",https://www.kaggle.com/crawford/crop-nutrient-database
b'(Better) - Donald Trump Tweets!',b'A collection of all of Donald Trump tweets--better than its predecessors',"b'# Context \nUnlike [This][1] dataset, (which proved to be unusable). And [This one][2] which was filled with unnecessary columns; This Donald trump dataset has the cleanest usability and consists of over 7,000 tweets, no nonsense\n\n**You may need to use a decoder other than UTF-8 if you want to see the emojis**\n# Content\n\n**Data consists of:**\n\n - \n\n -Date\n\n -Time\n\n -Tweet_Text\n\n -Type\n\n -Media_Type\n\n -Hashtags\n\n -Tweet_Id\n\n -Tweet_Url\n\n -twt_favourites_IS_THIS_LIKE_QUESTION_MARK\n\n -Retweets\n\nI scrapped this from someone on reddit\n===\n  [1]: https://www.kaggle.com/austinvernsonger/donaldtrumptweets\n  [2]: https://www.kaggle.com/benhamner/clinton-trump-tweets'","b""['internet', 'politics', 'small', 'featured']""",https://www.kaggle.com/kingburrito666/better-donald-trump-tweets
b'The Correlates of State Policy Project',b'One-stop shop for anyone studying state policies and politics',"b'### Context\n\nThe Correlates of State Policy Project aims to compile, disseminate, and encourage the use of data relevant to U.S. state policy research, tracking policy differences across and change over time in the 50 states. We have gathered more than nine-hundred variables from various sources and assembled them into one large, useful dataset. We hope this Project will become a \xe2\x80\x9cone-stop shop\xe2\x80\x9d for academics, policy analysts, students, and researchers looking for variables germane to the study of state policies and politics. \n\n### Content\n\nThe Correlates of State Policy Project includes more than nine-hundred variables, with observations across the U.S. 50 states and time (1900 \xe2\x80\x93 2016). These variables represent policy outputs or political, social, or economic factors that may influence policy differences across the states. The codebook includes the variable name, a short description of the variable, the variable time frame, a longer description of the variable, and the variable source(s) and notes.\n\nTake a look at the codebook PDF to get more information about each column\n\n\n### Acknowledgements\n\nThis aggregated data set is only possible because many scholars and students have spent tireless hours creating, collecting, cleaning, and making data publicly available. Thus if you use the dataset, please cite the original data sources. \n\nJordan, Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing, MI: Institute for Public Policy and Social Research (IPPSR). \n\nThis dataset was originally downloaded from \n\nhttp://ippsr.msu.edu/public-policy/correlates-state-policy'","b""['medium', 'featured']""",https://www.kaggle.com/ippsr/correlates-state-policy
b'Crime Statistics for South Africa',b'A history of crime statistics from 2004 to 2015 per province and station',"b'##Context\n\n**CRIME STATISTICS: INTEGRITY**\n\nThe South African Police Service (SAPS) has accepted a new and challeging objective of ensuring that its crime statistics are in line with international best practice. This will be achieved through a Memorandum of Understanding with Statistics South Africa (Stats SA), aimed at further enhancing the quality and integrity of the South African crime statistics. \n\nThe crime statistics generated by SAPS are an important link in the value chain of the statistics system informs policy development and planning in the criminal justice system. The collaboration with StatsSA will go a long way in enhancing the integrity of the SAPS crime statistics and ensuring that policy-makers have quality data to assist them with making policy decisions.\n\n##Content\n\nThe dataset contains South African crime statistics, broken down per province, station and crime type.\n\n##Acknowledgements\n\nData as published from:\n\n* http://www.saps.gov.za/resource_centre/publications/statistics/crimestats/2015/crime_stats.php\n\nFurther sources:\n\n* http://www.saps.gov.za/services/crimestats.php\n\nAn overview presentation:\n\n* http://www.saps.gov.za/services/final-crime-stats-release-02september2016.pdf'","b""['crime', 'medium', 'featured']""",https://www.kaggle.com/slwessels/crime-statistics-for-south-africa
b'Crop Residue Cover Measurement',b'On the ground or in the air? ',"b'### Context\nIn commercial agriculture it is common practice to retain so-called crop residue (agricultural product left on the field after a harvest) on planting fields. This is a commonly recommended practice in [conservation agriculture](https://en.wikipedia.org/wiki/Conservation_agriculture).  \n\nThis dataset is a measurement of the adoption and impact of the methodology for a selection of agricultural fields. Six different crop residue coverage measurement methods are included: i) interviewee (respondent) estimation; ii) enumerator estimation visiting the field; iii) interviewee with visual-aid without visiting the field; iv) enumerator with visual-aid visiting the field; v) field picture collected with a drone and analyzed with image-processing methods and vi) satellite picture of the field analyzed with remote sensing methods. \n\n### Content\n\nThis dataset is a `CSV` file with a selection of characteristics about fields included in the sample.\n\n### Acknowledgements\n\nThis dataset was created as part of the following study: [""On the Ground or in the Air? A Methodological Experiment on\nCrop Residue Cover Measurement in Ethiopia""](https://link.springer.com/content/pdf/10.1007%2Fs00267-017-0898-0.pdf).\n\n### Inspiration\n\nWhat are the characteristics of the Ethiopian farmers sampled in this dataset? How well do they follow conservation practices?'","b""['survey analysis', 'agriculture', 'africa', 'small', 'featured']""",https://www.kaggle.com/fkosmowski/crop-residue-cover-measurement
b'Freesound: Content-Based Audio Retrieval',b'Find sounds with text-queries based on their acoustic content',"b'# Context \n\nPeople have been making music for tens of thousands of years [1]. Today, making music is easier and more accessible than ever before. The technological developments of the last few decades allow people to simulate playing every imaginable instrument on their computers. Audio sequencers enable users to arrange their songs on a time line, sample by sample. Digital audio workstations (DAWs) ship with virtual instruments and synthesizers which allow users to virtually play a whole band or orchestra in their bedrooms.\n\nOne challenge in working with DAWs is organizing samples and recordings in a structured way; so, users can easily access them. In addition to their own recordings, many users download samples. Browsing through sample collections to find the perfect sound is time consuming and may impede the user\'s creative flow [2]. On top of this, manually naming and tagging recordings is a time-consuming and tedious task, so not many users do [3]. The consequence is that finding the right sound at the right moment becomes a challenging problem [4].\n\nModeling the relationship between the acoustic content and semantic descriptions of sounds could allow users to retrieve sounds using text queries. This dataset was collected to support research on content-based audio retrieval systems, focused on sounds used in creative context.\n\n\n# Content\n\nThis dataset was collected from [Freesound](http://freesound.org]) [5] in June 2016. It contains the frame-based MFCCs of about 230,000 sounds and the associated tags.\n\n- `sounds.json`: Sound metadata originally downloaded from the Freesound API. This file includes the `id`, associated `tags`, links to `previews`, and links to an `analysis_frames` file, which contains frame-based low-level features, for each sound.\n- `preprocessed_tags.csv`: Preprocessed tags. Contains only tags which are associated to at least 0.01% of sounds. Moreover, tags were split on hyphens and stemmed. Tags containing numbers and short tags with less than three characters were removed.\n- `queries.csv`: An aggregated query-log of real user-queries against the Freesound database, collected between May 11 and November 24 in 2016.\n- `preprocessed_queries.csv` Queries were preprocessed in the same way tags were preprocessed.\n- `*_mfccs.csv.bz2`: The original MFCCs for each sound, extracted from the URL provided in the `analysis_frames` field of `sounds.json`, split across ten files.\n- `cb_{512|1024|2048|4096}_sparse.pkl`: Codebook representation of sounds saved as sparse  `pd.DataFrame`. The first-order and second-order derivatives of the 13 MFCCs were appended to the MFCC feature vectors of each sound. All frames were clustered using K-Means (Mini-Batch K-Means) to find {512|1024|2048|4096} cluster centers. Each frame was, then, assigned to its closest cluster center and the counts used to represent a sound as a single {512|1024|2048|4096}-dimensional vector. \n\n\n# Acknowledgements\n\nThanks to the Music Technology Group of the *Universitat Pompeu Fabra* in Barcelona for creating and maintaining the Freesound [5] database and for providing the aggregated query-logs.\n\n\n# Inspiration\n\nWho can create the best content-based audio retrieval system measured by precision-at-*k* for values of *k* in {1, ..., 20} and mean average precision.\n\n\n# Getting started\n\nHere\'s the accompanying GitHub repository: https://github.com/dschwertfeger/cbar \n\n\n# References\n\n[1] N. L. Wallin and B. Merker, The Origins of Music. MIT Press, 2001.\n\n[2] M. Csikszentmihalyi, Flow: The Psychology of Optimal Experience. New York: Harper Perennial Modern Classics, 2008.\n\n[3] E. Pampalk, A. Rauber, and D. Merkl, ""Content-based organization and visualization of music archives"", in Proceedings of the tenth ACM international conference on Multimedia, 2002, pp. 570\xe2\x80\x93579.\n\n[4] T. Bertin-Mahieux, D. Eck, and M. Mandel, ""Automatic tagging of audio: The state-of-the-art"", Machine audition: Principles, algorithms and systems, pp. 334\xe2\x80\x93352, 2010.\n\n[5] F. Font, G. Roma, and X. Serra, ""Freesound technical demo"", 2013, pp. 411\xe2\x80\x93412.'","b""['linguistics', 'sound technology', 'large', 'featured']""",https://www.kaggle.com/dschwertfeger/freesound
"b'Emotion, Aging, and Sentiment Over Time'",b'Data from the Chronist project',"b'Chronist is a project to quantitatively monitor the emotional and physical changes of an individual over periods of time. My thesis is that if you can accurately show emotional or physical change over time, you can objectively pinpoint how an environmental change such as a career change, moving to a new city, starting or ending a relationship, or starting a new habit like going to the gym affected your physical and emotional health. This can lead to important insights on an individual level and for a population as a whole.\n\nIf you are interested in hearing more about this project, contributing your data, or collaborating, contact me at chris@cjroth.com.\n\nSee the [GitHub repository](https://github.com/cjroth/chronist) to read more about the tools that were used to generate the dataset.'","b""['linguistics', 'gerontology', 'medium', 'featured']""",https://www.kaggle.com/cjroth/chronist
b'Wage Estimates',b'Modeled wage estimates of average hourly wages',"b'###Context: \nThe Occupational Employment Statistics (OES) and National Compensation Survey (NCS) programs have produced estimates by borrowing from the strength and breadth of each survey to provide more details on occupational wages than either program provides individually. Modeled wage estimates provide annual estimates of average hourly wages for occupations by selected job characteristics and within \ngeographical location. The job characteristics include bargaining status (union and nonunion), part- and full-time work status, incentive- and time-based pay, and work levels by occupation.\n\nDirect estimates are based on survey responses only from the particular geographic area to which the estimate refers. In contrast, modeled wage estimates use survey responses from larger areas to fill in information for smaller areas where the sample size is not sufficient to produce direct estimates. Modeled wage estimates require the assumption that the patterns to responses in the larger area hold in the smaller area.\n\nThe sample size for the NCS is not large enough to produce direct estimates by area, occupation, and job characteristic for all of the areas for which the OES publishes estimates by area and occupation. The NCS sample consists of 6 private industry panels with approximately 3,300 establishments sampled per panel, and 1,600 sampled state and local government units. The OES full six-panel sample consists of nearly 1.2 million establishments. \n\nThe sample establishments are classified in industry categories based on the North American Industry Classification System (NAICS). Within an establishment, specific job categories are selected to represent broader occupational definitions. Jobs are classified according to the Standard Occupational Classification (SOC) system.\n\n###Content:\n**Summary**: Average hourly wage estimates for civilian workers in occupations by job characteristic and work levels. These data are available at the national, state, metropolitan, and nonmetropolitan area levels.\n\n**Frequency of Observations**: Data are available on an annual basis, typically in May. \n\n**Data Characteristics**: All hourly wages are published to the nearest cent.\n\n###Acknowledgements: \nThis dataset was taken directly from the Bureau of Labor Statistics and converted to CSV format. \n\n###Inspiration: \nThis dataset contains the estimated wages of civilian workers in the United States. Wage changes in certain industries may be indicators for growth or decline. Which industries have had the greatest increases in wages? Combine this dataset with the Bureau of Labor Statistics Consumer Price Index dataset and find out what kinds of jobs you would need to afford your snacks and instant coffee!'","b""['income', 'medium', 'featured']""",https://www.kaggle.com/bls/wage-estimates
b'Weekly Dairy Product Prices',b'USDA data on bulk dairy production since 2010',"b""We don't always think about industrial scale food, but cheese blocks the size of a small car are important.\n\nThe Mandatory Price Reporting Act of 2010 (pdf) was passed on September 27, 2010, the act required USDA to release dairy product sales information on or before Wednesday at 3:00 pm EST (unless affected by a Federal Holiday). The act also required the establishment of an electronic mandatory price reporting system for dairy products reported under Public Law 106-532. These dairy statistics will continue to be collected on a weekly basis, AMS-Dairy Programs will collect, analyze, aggregate, and publish dairy product sales information for selected dairy commodities.\n\n### Acknowledgements\n\nThis data is released by the US Department of Agriculture. You can find [the original dataset here][1]. \n\n### Inspiration\n\n- Can you predict changes in moisture content for 40 pound blocks of cheese?\n\n\n  [1]: https://mpr.datamart.ams.usda.gov/menu.do?path=Products%5CDairy""","b""['finance', 'agriculture', 'small', 'featured']""",https://www.kaggle.com/sohier/weekly-dairy-product-prices
b'Pesticide Data Program (2013)',b'Study of pesticide residues in food',"b'# Context\n\nThis dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately, if EPA determines a pesticide is not safe for human consumption, it is removed from the market.\n\nThe PDP tests a wide variety of domestic and imported foods, with a strong focus on foods\nthat are consumed by infants and children. EPA relies on PDP data to conduct dietary risk\nassessments and to ensure that any pesticide residues in foods remain at safe levels. USDA\nuses the data to better understand the relationship of pesticide residues to agricultural practices\nand to enhance USDA\xe2\x80\x99s Integrated Pest Management objectives. USDA also works with U.S.\ngrowers to improve agricultural practices.\n\n# Content\n\nWhile the original 2013 MS Access database can be found [here](https://www.ams.usda.gov/datasets/pdp/pdpdata), the data has been transferred to a SQLite database for easier, more open use. The database contains two tables, Sample Data and Results Data. Each sampling includes attributes such as extraction method, the laboratory responsible for the test, and EPA tolerances among others. These attributes are labeled with codes, which can be referenced in PDF format [here](https://www.ams.usda.gov/datasets/pdp/pdpdata), or integrated into the database using the included csv files. \n\n# Inspiration\n\n* What are the most common types of pesticides tested in this study?\n* Do certain states tend to use one particular pesticide type over another?\n* Does pesticide type correspond more with crop type or location (state)?\n* Are any produce types found to have higher pesticide levels than assumed safe by EPA standards?\n* By combining databases from several years of PDP tests, can you see any trends in pesticide use?\n\n# Acknowledgement\n\nThis dataset is part of the USDA PDP yearly database, and the original source can be found [here](https://www.ams.usda.gov/datasets/pdp/pdpdata).'","b""['food and drink', 'agriculture', 'medium', 'featured']""",https://www.kaggle.com/usdeptofag/pesticide-data-program-2013
b'WWI Bombing Operations',b'Details on 1441 Allied Runs',"b'### Context: \nTHOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data.  Each theater of warfare has a separate data file, in addition to a [THOR Overview](http://www.data.mil/s/v2/data-stories-an-overview-of-thor/a100cd16-c2a7-453b-8ea6-45947c1bbc51/).\n\n### Content: \nThe THOR data contains data for Allied aircraft carrying a combined bomb load of more than a million pounds over 1,437 recorded missions. This Theater History of Operations (THOR) dataset combines digitized paper mission reports from WWI. It can be searched by date, conflict, geographic location and more than 30 additional data attributes forming a live-action sequence of events. See the data dictionary [here](https://www.dds.mil/data/thor_data_dictionary_2016.pdf) and additonal background [here](https://insight.livestories.com/s/v2/thor-world-war-i/5be11be2-83c7-4d20-b5bc-05b3dc325d7e/).\n\n### Acknowledgements: \nTHOR is a dataset project initiated by  Lt Col Jenns Robertson and continued in partnership with Data.mil,  an experimental project, created by the [Defense Digital Service](https://www.dds.mil/) in collaboration with the [Deputy Chief Management Officer]( http://dcmo.defense.gov/) and data owners throughout the U.S. military. \n\n\n### Inspiration: \n* Can you create animated maps of certain campaigns? See, for example, this [map of the Argonne-Meuse offensive](https://deptofdefense.github.io/data-mil-THOR-example/).\n* Can you match weather data with campaigns?\n* Where were the most campaigns?'","b""['war', 'military', 'small', 'featured']""",https://www.kaggle.com/usaf/wwi-bombing-operations
b'SeedLing',b'A Seed Corpus for the Human Language Project',"b""### Context\n\nA broad-coverage corpus such as the [Human Language Project envisioned by Abney and Bird (2010)](http://www.anthology.aclweb.org/P/P10/P10-1010.pdf) would be a powerful resource for the study of endangered languages.\nSeedLing was created as a seed corpus for the Human Language Project to cover a broad range of languages (Guy et al. 2014).\n\nTAUS (Translation Automation User Society) also see the [importance of the Human Language Project in the context of keeping up with the demand for capacity and speed for translation](https://www.taus.net/think-tank/articles/translate-articles/the-call-for-the-human-language-project).  TAUS' definition of the Human Language Project can be found on https://www.taus.net/knowledgebase/index.php/Human_Language_Project\n\nA detailed explanation of how to use the corpus can be found on https://github.com/alvations/SeedLing\n\n### Content\n\nThe SeedLing corpus on this repository includes the data from:\n\n - **ODIN**: Online Database of Interlinear Text\n - **Omniglot**: Useful foreign phrases from www.omniglot.com\n - **UDHR**: Universal Declaration of Human Rights\n\n### Acknowledgements\n**Citation**:\n\nGuy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer and Michaela Regneri . 2014. SeedLing: Building and using a seed corpus for the Human Language Project. In Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop. Baltimore, USA.\n\n    @InProceedings{seedling2014,\n      author    = {Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer and Michaela Regneri},\n      title     = {SeedLing: Building and using a seed corpus for the Human Language Project},\n      booktitle = {Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop},\n      month     = {June},\n      year      = {2014},\n      address   = {Baltimore, USA},\n      publisher = {Association for Computational Linguistics},\n      pages     = {},\n      url       = {}\n    }\n\n**References**:\n\n    Steven Abney and Steven Bird. 2010. The Human Language \n    Project: Building a universal corpus of the world\xe2\x80\x99s languages. \n    In Proceedings of the 48th Annual Meeting of the Association \n    for Computational Linguistics, pages 88\xe2\x80\x9397.\n\n    Sime Ager. Omniglot - writing systems and languages \n    of the world. Retrieved from www.omniglot.com.\n    \n    William D Lewis and Fei Xia. 2010. Developing ODIN: A multilingual \n    repository of annotated language data for hundreds of the world\xe2\x80\x99s \n    languages. Literary and Linguistic Computing, 25(3):303\xe2\x80\x93319.\n    \n    UN General Assembly, Universal Declaration of Human Rights, \n    10 December 1948, 217 A (III), available at: \n    http://www.refworld.org/docid/3ae6b3712c.html \n    [accessed 26 April 2014]\n\n### Inspiration\n\nThis corpus was created in a span a semester in Saarland University by a linguist, a mathematician, a data geek and two amazing mentors from the [COLI department](http://www.coli.uni-saarland.de/). It wouldn't have been possible without the cross-disciplinary synergy and the common goal we had.\n\n - Expand/Explore the Human Language Project.\n - Go to the field and record/document their language. Make them computationally readable.\n - Grow the Seedling!""","b""['linguistics', 'languages', 'culture and humanities', 'small', 'featured']""",https://www.kaggle.com/alvations/seedling
b'City Lines',"b""Explore the transport systems of the world's cities""","b'### Context\n \nWhat did the expansion of the London Underground, the world\xe2\x80\x99s first underground railway which opened in 1863, look like? What about the transportation system in your home city? Citylines collects data on transportation lines across the world so you can answer questions like these and more.\n \n### Content\n \nThis dataset, originally shared and updated [here][1], includes transportation line data from a number of cities from around the world including London, Berlin, Mexico City, Barcelona, Washington D.C., and others covering many thousands of kilometers of lines. \n \n### Inspiration\n \nYou can explore geometries to generate maps and even see how lines have changed over time based on historical records. Want to include shapefiles with your analysis? Simply [publish a shapefile dataset here][2] and then [create a new kernel][3] (R or Python script/notebook), adding your shapefile as an additional datasource.\n \n\n\n  [1]: http://www.citylines.co/data\n  [2]: http://www.citylines.co/data\n  [3]: https://www.kaggle.com/citylines/city-lines/kernels?modal=true'","b""['cities', 'transport', 'small', 'featured']""",https://www.kaggle.com/citylines/city-lines
b'Cervical Cancer Risk Classification',b'prediction of  cancer indicators; Please download; run kernel & upvote',"b'Cervical Cancer Risk Factors for Biopsy: This Dataset is Obtained from UCI Repository and kindly acknowledged!\n\nThis file contains a List of Risk Factors for Cervical Cancer leading to a Biopsy Examination!\n\nAbout 11,000 new cases of invasive cervical cancer are diagnosed each year in the U.S. However, the number of new cervical cancer cases has been declining steadily over the past decades. Although it is the most preventable type of cancer, each year cervical cancer kills about 4,000 women in the U.S. and about 300,000 women worldwide. In the United States, cervical cancer mortality rates plunged by 74% from 1955 - 1992 thanks to increased screening and early detection with the Pap test. AGE Fifty percent of cervical cancer diagnoses occur in women ages 35 - 54, and about 20% occur in women over 65 years of age. The median age of diagnosis is 48 years. About 15% of women develop cervical cancer between the ages of 20 - 30. Cervical cancer is extremely rare in women younger than age 20. However, many young women become infected with multiple types of human papilloma virus, which then can increase their risk of getting cervical cancer in the future. Young women with early abnormal changes who do not have regular examinations are at high risk for localized cancer by the time they are age 40, and for invasive cancer by age 50. SOCIOECONOMIC AND ETHNIC FACTORS Although the rate of cervical cancer has declined among both Caucasian and African-American women over the past decades, it remains much more prevalent in African-Americans -- whose death rates are twice as high as Caucasian women. Hispanic American women have more than twice the risk of invasive cervical cancer as Caucasian women, also due to a lower rate of screening. These differences, however, are almost certainly due to social and economic differences. Numerous studies report that high poverty levels are linked with low screening rates. In addition, lack of health insurance, limited transportation, and language difficulties hinder a poor woman\xe2\x80\x99s access to screening services. HIGH SEXUAL ACTIVITY Human papilloma virus (HPV) is the main risk factor for cervical cancer. In adults, the most important risk factor for HPV is sexual activity with an infected person. Women most at risk for cervical cancer are those with a history of multiple sexual partners, sexual intercourse at age 17 years or younger, or both. A woman who has never been sexually active has a very low risk for developing cervical cancer. Sexual activity with multiple partners increases the likelihood of many other sexually transmitted infections (chlamydia, gonorrhea, syphilis).Studies have found an association between chlamydia and cervical cancer risk, including the possibility that chlamydia may prolong HPV infection. FAMILY HISTORY Women have a higher risk of cervical cancer if they have a first-degree relative (mother, sister) who has had cervical cancer. USE OF ORAL CONTRACEPTIVES Studies have reported a strong association between cervical cancer and long-term use of oral contraception (OC). Women who take birth control pills for more than 5 - 10 years appear to have a much higher risk HPV infection (up to four times higher) than those who do not use OCs. (Women taking OCs for fewer than 5 years do not have a significantly higher risk.) The reasons for this risk from OC use are not entirely clear. Women who use OCs may be less likely to use a diaphragm, condoms, or other methods that offer some protection against sexual transmitted diseases, including HPV. Some research also suggests that the hormones in OCs might help the virus enter the genetic material of cervical cells. HAVING MANY CHILDREN Studies indicate that having many children increases the risk for developing cervical cancer, particularly in women infected with HPV. SMOKING Smoking is associated with a higher risk for precancerous changes (dysplasia) in the cervix and for progression to invasive cervical cancer, especially for women infected with HPV. IMMUNOSUPPRESSION Women with weak immune systems, (such as those with HIV / AIDS), are more susceptible to acquiring HPV. Immunocompromised patients are also at higher risk for having cervical precancer develop rapidly into invasive cancer. DIETHYLSTILBESTROL (DES) From 1938 - 1971, diethylstilbestrol (DES), an estrogen-related drug, was widely prescribed to pregnant women to help prevent miscarriages. The daughters of these women face a higher risk for cervical cancer. DES is no longer prsecribed.'","b""['healthcare', 'human genetics', 'small', 'featured']""",https://www.kaggle.com/loveall/cervical-cancer-risk-classification
b'News Aggregator Dataset',b'Headlines and categories of 400k news stories from 2014',"b""This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014.\n\nNews categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.\n\n## Content\nThe columns included in this dataset are:\n\n- **ID** : the numeric ID of the article\n- **TITLE** : the headline of the article\n- **URL** : the URL of the article\n- **PUBLISHER** : the publisher of the article\n- **CATEGORY** : the category of the news item; one of:\n-- *b* : business\n-- *t* : science and technology\n-- *e* : entertainment\n-- *m* : health\n- **STORY** : alphanumeric ID of the news story that the article discusses\n- **HOSTNAME** : hostname where the article was posted\n- **TIMESTAMP** : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)\n\n## Acknowledgments\nThis dataset comes from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Any publications that use this data should cite the repository as follows:\n\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n\nThis specific dataset can be found in the UCI ML Repository at [this URL](http://archive.ics.uci.edu/ml/datasets/News+Aggregator)\n\n## Inspiration\nWhat kinds of questions can we explore using this dataset? Here are a few possibilities:\n\n- can we predict the category (business, entertainment, etc.) of a news article given only its headline?\n- can we predict the specific story that a news article refers to, given only its headline?""","b""['linguistics', 'news agencies', 'medium', 'featured']""",https://www.kaggle.com/uciml/news-aggregator-dataset
b'SciRate quant-ph',b'Papers published in arXiv/quant-ph between 2012-2016 with number of Scites',"b'# Context \n\nI was curious about the hot topics in quantum physics as reflected by the [quant-ph](https://arxiv.org/archive/quant-ph) category on arXiv. Citation counts have a long lag, and so do journal publications, and I wanted a more immediate measure of interest. [SciRate](http://scirate.com/) is fairly well known in this community, and I noticed that after the initial two-three weeks, the number of Scites a paper gets hardly increases further. So the number of Scites is both immediate and near constant after a short while. \n\n# Content\n\nThe main dataset (`scirate_quant-ph.csv`) is the metadata of all papers published in quant-ph between 2012-01-01 and 2016-12-31 that had at least ten Scites, as crawled on 2016-12-31. It has six columns:\n\n- The id column as exported by pandas.\n- The arXiv id.\n- The year of publication.\n- The month of publication.\n- The day of publication.\n- The number of Scites (this column defines the order).\n- The title.\n- All authors separates by a semicolon.\n- The abstract.\n\nThe author names were subjected to normalization and the chances are high that the same author only appears with a unique name.\n\nThe name normalization was the difficult part in compiling this collection, and this is why the number of Scites was lower bounded. A second file (`scirate_quant-ph_unnormalized.csv`) includes all papers that appeared between 2012-2016 irrespective of the number of Scites, but the author names are not normalized. The actual number of Scites for each paper may show a slight variation between the two datasets because the unnormalized version was compiled more than a month later.\n\n# Acknowledgements\n\nMany thanks to SciRate for tolerating my crawling trials and not blacklisting my IP address.\n\n\n# Inspiration\n\nUnleash topic models and author analysis to find out what or who is hot in quantum physics today. Build a generative model to write trendy fake titles like [SnarXiv](http://snarxiv.org/) does it for hep-th.'","b""['linguistics', 'research', 'physics', 'medium', 'featured']""",https://www.kaggle.com/peterwittek/scirate-quant-ph
"b'Airports, Train Stations, and Ferry Terminals'","b""Openflight.org's database of the worlds transportation hubs""","b'### Context\n\nThis is a database of airports, train stations, and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.\n\n### Content\n\n- Airport ID\tUnique OpenFlights identifier for this airport.\n- Name\tName of airport. May or may not contain the City name.\n- City\tMain city served by airport. May be spelled differently from Name.\n- Country\tCountry or territory where airport is located. See countries.dat to cross-reference to ISO 3166-1 codes.\n- IATA\t3-letter IATA code. Null if not assigned/unknown.\n- ICAO\t4-letter ICAO code.\n- Null if not assigned.\n- Latitude\tDecimal degrees, usually to six significant digits. Negative is South, positive is North.\n- Longitude\tDecimal degrees, usually to six significant digits. Negative is West, positive is East.\n- Altitude\tIn feet.\n- Timezone\tHours offset from UTC. Fractional hours are expressed as decimals, eg. India is 5.5.\n- DST\tDaylight savings time. One of E (Europe), A (US/Canada), S (South America), O (Australia), Z (New Zealand), N (None) or U (Unknown). See also: Help: Time\n- Tz database time zone\tTimezone in ""tz"" (Olson) format, eg. ""America/Los_Angeles"".\n- Type\tType of the airport. Value ""airport"" for air terminals, ""station"" for train stations, ""port"" for ferry terminals and ""unknown"" if not known.\n- Source\tSource of this data. ""OurAirports"" for data sourced from OurAirports, ""Legacy"" for old data not matched to OurAirports (mostly DAFIF), ""User"" for unverified user contributions. In airports.csv, only source=OurAirports is included.\n\n\n### Acknowledgements\n\nThis dataset was downloaded from [Openflights.org][1] under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out! \n\n  [1]: https://openflights.org/data.html'","b""['vehicles', 'transport', 'aviation', 'public transport', 'rail transport', 'small', 'featured']""",https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals
b'Expanded HR Analytics Data Lab',b'To use for various exercises including multivariate analysis.',"b""### Context\n\nWe are building an HR analytics data set that can be used for building useful reports,  understanding the difference between data and information, and multivariate analysis. The data set we are building is similar to that used in several academic reports and what may be found in ERP HR subsystems.\n\nWe will update the sample data set as we gain a better understanding of the data elements using the calculations that exist in scholarly journals. Specifically, we will use the correlation tables to rebuild the data sets.\n\n### Content\n\nThe fields represent a fictitious data set where a survey was taken and actual employee metrics exist for a particular organization. None of this data is real.\n\n### Acknowledgements\n\nWe wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research. \n\nPrabhjot Singh contributed a portion of the data (the columns on the right before the survey data was added).\nhttps://www.kaggle.com/prabhjotindia\nhttps://www.kaggle.com/prabhjotindia/visualizing-employee-data/data\n\nAbout this Dataset\nWhy are our best and most experienced employees leaving prematurely? Have fun with this database and try to predict which valuable employees will leave next. Fields in the dataset include:\n\nSatisfaction Level\nLast evaluation\nNumber of projects\nAverage monthly hours\nTime spent at the company\nWhether they have had a work accident\nWhether they have had a promotion in the last 5 years\nDepartments\nSalary\n\n\n### Inspiration\n\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?""","b""['small', 'featured']""",https://www.kaggle.com/krismurphy01/data-lab
b'NASA Facilities',b'A dataset of NASA facility names and locations',"b'### Context\n\nNASA has something like 400 different facilities across the United States! This dataset is a collection of those facilities and their locations.\n\n### Content\n\n- Center: Name of the ""Center"", a collection facilities\n- Center Search Status: Public or...?\n- Facility: Name of the facility\n- FacilityURL\n- Occupied\n- Status\n- URL Link\n- Record Date\n- Last Update\n- Country\n- Location\n- City\n- State\n- Zipcode\n\n\n### Acknowledgements\n\nThis dataset was downloaded from [https://data.nasa.gov/Management-Operations/NASA-Facilities/gvk9-iz74][1]. The original file was modified to remove contact information for each facility.\n\n\n  [1]: https://data.nasa.gov/Management-Operations/NASA-Facilities/gvk9-iz74'","b""['space', 'spaceflight', 'organizations', 'small', 'featured']""",https://www.kaggle.com/nasa/nasa-facilities
b'Ethereum Historical Data',b'All Ethereum data from the start to August 2018',"b""# Context \n\nThe Ethereum blockchain gives a revolutionary way of decentralized applications and provides its own cryptocurrency. Ethereum is a  decentralized platform that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third party interference. These apps run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. This enables developers to create markets, store registries of debts or promises, move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet, all without a middle man or counterparty risk.\n# Content\n\nWhat you may see in the CSVs are just numbers, but there is more to this.\n**Numbers make machine learning easy.** I've labeled each column, the first in all of them is the day; it may look weird but it makes sense if you look closely. \n\n\n## Note:##\n**TIMESTAMP FORMAT**\n#How to convert timestamp in python:\n\n    import datetime as dt\n    # The (would-be) timestamp value is below\n    timestamp = 1339521878.04 \n    # Technechly you would iterate through and change them all if you were graphing\n    timeValue = dt.datetime.fromtimestamp(timestamp)\n    #Year, month, day, hour, minute, second\n    print(timeValue.strftime('%Y-%m-%d %H:%M:%S'))\n\n# Acknowledgements\n\nMR. Vitalik Buterin. co-founder of Ethereum and as a co-founder of Bitcoin Magazine.\n\nHit a brother up\n===\n**0x767e8b211f70c5b8b4caa38c2efe05bf8eac0da7**\n===\n# Will be updating every month with new Ethereum history!""","b""['finance', 'history', 'small', 'featured']""",https://www.kaggle.com/kingburrito666/ethereum-historical-data
b'Six Degrees of Francis Bacon',b'An early modern social network',"b'### Overview\n\nSix Degrees of Francis Bacon is a digital reconstruction of the early modern social network (EMSN). Historians and literary critics have long studied the way that early modern people associated with each other and participated in various kinds of formal and informal groups. By data-mining existing scholarship that describes relationships between early modern persons, documents, and institutions, we have created a unified, systematized representation of the way people in early modern England were connected.\n\n### Contents\n\nThis dataset contains information on 171419 relationships between 15824 early modern figures (including, of course, the titular Francis Bacon). The individuals have been separated into 109 distinct labelled groups and the relationships fall under one of 64 labelled categories.\n\nThis dataset contains the following files:\n\n* SDFB_groups.csv: a list of the groups of individuals in the dataset\n* SDFB_people.csv: a list of all individuals in the dataset\n* SDFB_relationships_100000000_100020000.csv: This and the following \xe2\x80\x9crelationships\xe2\x80\x9d files contain information on relationships between specific individuals\n* SDFB_relationships_100020001_100040000.csv\n* SDFB_relationships_100040001_100060000.csv\n* SDFB_relationships_100060001_100080000.csv\n* SDFB_relationships_100080001_100100000.csv\n* SDFB_relationships_100100001_100120000.csv\n* SDFB_relationships_100120001_100140000.csv\n* SDFB_relationships_100140001_100160000.csv\n* SDFB_relationships_100160001_100180000.csv\n* SDFB_relationships_greater_than_100180000.csv\n* SDFB_RelationshipTypes.csv: the types of relationships found in the database\n* table-of-contents.csv: a table of contents for the files\n\n### Acknowledgements\n\nPlease cite Six Degrees of Francis Bacon as follows:\n\nSDFB Team, Six Degrees of Francis Bacon: Reassembling the Early Modern Social Network. www.sixdegreesoffrancisbacon.com (August 29, 2017).\n\n### Inspiration\n\nThis dataset is an excellent place to explore network analysis and visualization. Each individual is a node, and each relationship an edge. \n\n* Can you visualize this social network? \n* Who is the most central figure in this social network? \n* Do different groups have different degrees of connectivity? Plexity?'","b""['europe', 'history', 'sociology', 'networks', 'north america', 'medium', 'featured']""",https://www.kaggle.com/rtatman/six-degrees-of-francis-bacon
b'PTM Strava Data',b'GPX files from a GPS device',b'### Context\n\nSelected activities from Strava for use in Kaggle Kernels only.  GPX files from a GPS device.\n\n### Content\n\nLaramie_Enduro_2014.gpx\t\n\nLaramie_Enduro_2015.gpx\t\n\nLaramie_Enduro_2016.gpx\t\n\nWest_Laramie_Bike_Ride.gpx\t\n\nColorado_Belford_Oxford_and_Missouri_Mountains_Hike.gpx\t\n\nColorado_Longs_Peak_and_Chasm_Lake_Hike.gpx\t\n\nBerenthanti_Ghorepani_Ghandruk_Loop_Hike_Day_1_of_3_.gpx\t\n\nBerenthanti_Ghorepani_Ghandruk_Loop_Hike_Day_2_of_3_.gpx\t\n\nBerenthanti_Ghorepani_Ghandruk_Loop_Hike_Day_3_of_3_.gpx\t\n\n\n\n### Acknowledgements\n\nStrava\n\n### Inspiration\n\nStrava Labs',"b""['data visualization', 'geospatial analysis', 'small', 'featured']""",https://www.kaggle.com/paultimothymooney/ptm-strava-data
"b""Ben Hamner's Tweets""","b""A complete Twitter timeline from Kaggle's CTO""","b""### Context\n\nI was looking for something Ben Hamner, Kaggle's CTO, tweeted a while back and it turned out just using R's `TwitteR` package was easier than scrolling through his timeline. Since I collected all of his tweets, I figured I would share them here as well.\n\n### Content\n\n**What you get**: All of Ben Hamner's tweets current through today (12 December 2017).\n\n**What's inside**: The text from his tweets plus metadata like favorites, retweets, timestamps, etc. You can even see whether or not I've personally `favorited` or `retweeted` his tweets.\n\n### Acknowledgements\n\nThanks to Ben for his insightful tweets! Check out his tweets at [@benhamner on Twitter][1].\n\n\n  [1]: https://twitter.com/benhamner""","b""['internet', 'linguistics', 'twitter', 'small', 'featured']""",https://www.kaggle.com/mrisdal/ben-hamners-tweets
b'Association of Tennis Professionals Matches',b'ATP tournament results from 2000 to 2017',"b""# Context \n\nA dataset of ATP matches including individual statistics.\n\n\n# Content\n\nIn these datasets there are individual csv files for ATP tournament from 2000 to 2017.\n\nThe numbers in the last columns are absolute values, using them you can calculate percentages.\n\n##Dataset legend\n\nAll the match statistics are in absolute number format, you can convert to percentages using the total point number\n\n    ace = absolute number of aces\n    df = number of double faults\n    svpt = total serve points\n    1stin = 1st serve in\n    1st won = points won on 1st serve\n    2ndwon = points won on 2nd serve\n    SvGms = serve games\n    bpSaved = break point saved\n    bpFaced = break point faced\n\n# Acknowledgement\n\nThanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile\n\nhttps://github.com/JeffSackmann/tennis_atp\n\n\n# Inspiration\n\nThis dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them.""","b""['sports', 'tennis', 'medium', 'featured']""",https://www.kaggle.com/gmadevs/atp-matches-dataset
b'Seattle Airbnb Open Data',"b'A sneak peek into the Airbnb activity in Seattle, WA, USA'","b'# Context\n\nSince 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in Seattle, WA. \n\n# Content\n\nThe following Airbnb activity is included in this Seattle dataset:\n* Listings, including full descriptions and average review score\n* Reviews, including unique id for each reviewer and detailed comments\n* Calendar, including listing id and the price and availability for that day\n\n# Inspiration\n\n* Can you describe the vibe of each Seattle neighborhood using listing descriptions?\n* What are the busiest times of the year to visit Seattle? By how much do prices spike?\n* Is there a general upward trend of both new Airbnb listings and total Airbnb visitors to Seattle?\n\nFor more ideas, visualizations of all Seattle datasets can be found [here](http://insideairbnb.com/seattle/).\n\n# Acknowledgement\n\nThis dataset is part of Airbnb Inside, and the original source can be found [here](http://insideairbnb.com/get-the-data.html).'","b""['united states', 'hotels', 'home', 'medium', 'featured']""",https://www.kaggle.com/airbnb/seattle
b'Inception V3 Model',b'Inception V3 Tensorflow Model',"b'[Inception-v3](https://arxiv.org/abs/1512.00567) is trained for the [ImageNet](http://image-net.org/) Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into [1000 classes](http://image-net.org/challenges/LSVRC/2014/browse-synsets), like ""Zebra"", ""Dalmatian"", and ""Dishwasher"".\n\nHere\'s [code on GitHub to train Inception-v3](https://github.com/tensorflow/models/tree/master/inception)'","b""['artificial intelligence', 'medium', 'featured']""",https://www.kaggle.com/google-brain/inception-v3
b'Noun Compositionality Judgements',b'Is a flea market a market for fleas?',"b'### Context: \n\xe2\x80\x9cCompositionality\xe2\x80\x9d is a concept from linguistics where the meaning of a phrase is made up of the meaning of each of its individual words. So \xe2\x80\x9cthe red apple\xe2\x80\x9d refers, literally, to an apple that is red. Sometimes when you combine words, however, the meaning of the phrase isn\xe2\x80\x99t the same as the combined meanings of the individual words. \xe2\x80\x9cThe Big Apple\xe2\x80\x9d, for example, means \xe2\x80\x9cNew York City\xe2\x80\x9d, not a literal large apple.\n\nThis dataset contains human judgements of the compositionality of common English phrases with two nouns.\n\n### Content: \nThis dataset contains two files: summary data of the human judgements and the annotations by individual judges. All judgements are on a scale of 0 to 5, with 0 being \xe2\x80\x9cnot literal\xe2\x80\x9d and 5 being \xe2\x80\x9cliteral\xe2\x80\x9d.\n\n### Acknowledgements:\nThis dataset was collected by Siva Reddy, Diana McCarthy and Suresh Manandhar. If you use this dataset in your work, please cite the following paper:\n\nReddy, S., McCarthy, D., & Manandhar, S. (2011, November). An Empirical Study on Compositionality in Compound Nouns. In IJCNLP (pp. 210-218).\n\n### Inspiration: \n\n* Is there a relationship between how frequent a word is and how often it\xe2\x80\x99s used literally? \n(You can estimate word frequency using the corpora included in the [Natural Language Toolkit](http://www.nltk.org/), which is already ready to run in any Python kernel!)\n* Given this dataset, can you predict whether other compound nouns will be literal or not?\n* Which phrases are the most literal? Which are the least literal? Are there any patterns you notice?'","b""['linguistics', 'languages', 'small', 'featured']""",https://www.kaggle.com/rtatman/noun-compositionality-judgements
b'Worldwide Economic Remittances',b'Money sent home to family by workers abroad',"b'### Context\n\nIn 2013 alone, international migrants sent $413 billion home to families and friends. This money is known as ""remittance money"", and the total is more than three times that afforded by total global foreign aid ($135 billion). Remittances are traditionally associated with poor migrants moving outside of their home country to find work, supporting their families back home on their foreign wages; as a result, they make up a significant part of the economic picture for many developing countries in the world.\n\nThis dataset, published by the World Bank, provides estimates of 2016 remittance movements between various countries. It also provides historical data on the flow of such money going back to 1970.\n\nFor a look at how remittances play into the global economy, watch ""[The hidden force in global economics: sending money home](https://www.ted.com/talks/dilip_ratha_the_hidden_force_in_global_economics_sending_money_home?language=en)"".\n\n### Content\n\nThis dataset contains three files:\n\n* `bilateral-remittance.csv` --- Estimated remittances between world countries in the year 2016.\n* `remittance-inflow.csv` --- Historical remittance money inflow into world countries since 1970. Typically high in developing nations.\n* `remittance-outflow.csv` --- Historical remittance money outflow from world countries since 1970. Typically high in more developed nations.\n\nAll monetary values are in terms of millions of US dollars.\n\nFor more information on how this data was generated and calculated, refer to the [World Bank Remittance Data FAQ](http://www.worldbank.org/en/topic/migrationremittancesdiasporaissues/overview).\n\n### Acknowledgements\n\nThis dataset is a republished version of three of the tables published by the World Bank which has been slightly cleaned up for use on Kaggle. For the original source, and other complimentary materials, check out the [dataset home page](http://www.worldbank.org/en/topic/migrationremittancesdiasporaissues/brief/migration-remittances-data).\n\n### Inspiration\n\n* What is the historical trend in remittance inflows and outflows for various countries? How does this relate to the developmental character of the countries in question?\n* What countries send to most money abroad? What countries receive the most money from abroad? Try combining this dataset with a demographics dataset to see what countries are most and least reliant on income from abroad.\n* How far do workers migrate for a job? Are they staying near home, or going half the world away? Are there any surprising facts about who send money to who?'","b""['economics', 'demographics', 'countries', 'international relations', 'globalization', 'small', 'featured']""",https://www.kaggle.com/theworldbank/worldwide-economic-remittances
b'Porter Test',b'Regression test to compare NLTK vs Tatarus Porter stemmer',"b'### Context\n\nThis test set was created as a regression test to illustrate the differences in the Porter stemmer variants implemented in NLTK. This was added in [issue 1261](https://github.com/nltk/nltk/pull/1261) during a major rewrite of the Porter stemmer by [Mark Amery](https://github.com/ExplodingCabbage). The details of the unit test can be found in [`nltk.test.unit.test_stem.py`](https://github.com/nltk/nltk/blob/develop/nltk/test/unit/test_stem.py)\n\n### Content\n\n - **porter_vocabulary.txt**: Input words used in the regression test. \n - **porter_martin_output.txt**: Output of the [updated Porter stemmer implemented by Martin Porter](https://tartarus.org/martin/PorterStemmer/)\n - **porter_original_output.txt**: Output of the [original ""Martin-blessed"" Porter stemmer written in C](http://tartarus.org/martin/PorterStemmer/c.txt)\n - **porter_nltk_output.txt**: Output of the Porter stemmer implemented in NLTK\n\n\n### Acknowledgements\n\nCredits go to Mark Amery for creating this regression test for NLTK version of the Porter stemmer as well as the major rewrite of the stemmer itself!!'","b""['small', 'featured']""",https://www.kaggle.com/nltkdata/porter-test
b'Corruption Perceptions Index',b'Perceived level of political corruption for every country in the world',"b'# Content\n\nThe Corruption Perceptions Index scores and ranks countries/territories based on how corrupt a country\xe2\x80\x99s public sector is perceived to be. It is a composite index, a combination of surveys and assessments of corruption, collected by a variety of reputable institutions. The CPI is the most widely used indicator of corruption worldwide.\n\nCorruption generally comprises illegal activities, which are deliberately hidden and only come to light through scandals, investigations or prosecutions. There is no meaningful way to assess absolute levels of corruption in countries or territories on the basis of hard empirical data. Possible attempts to do so, such as by comparing bribes reported, the number of prosecutions brought or studying court cases directly linked to corruption, cannot be taken as definitive indicators of corruption levels. Instead, they show how effective prosecutors, the courts or the media are in investigating and exposing corruption. Capturing perceptions of corruption of those in a position to offer assessments of public sector corruption is the most reliable method of comparing relative corruption levels across countries.\n\n\n# Acknowledgements\n\nThe data sources used to calculate the Corruption Perceptions Index scores and ranks were provided by the African Development Bank, Bertelsmann Stiftung Foundation, The Economist, Freedom House, IHS Markit, IMD Business School, Political and Economic Risk Consultancy, Political Risk Services, World Bank, World Economic Forum, World Justice Project, and Varieties of Democracy Project.'","b""['crime', 'politics', 'small', 'featured']""",https://www.kaggle.com/transparencyint/corruption-index
b'French presidential election',b'Extract from Twitter about the French election (with a taste of Google Trends)',"b'# Context \n\nThis dataset is born from a test with the twitter streaming api to filter and collect data from this flow on a specific topic, in this case the [French election][1].The script used to make this data collection is available on this [Github repository][2].\n\nSince the 18th of March, the [French election][3] enter in the final straight line until the first poll the 23 April 2017 , the candidates for the position are:\n\n - M. Nicolas DUPONT-AIGNAN\n - Mme Marine LE PEN\n - M. Emmanuel MACRON\n - M. Beno\xc3\xaet HAMON\n - Mme Nathalie ARTHAUD\n - M. Philippe POUTOU\n - M. Jacques CHEMINADE\n - M. Jean LASSALLE\n - M. Jean-Luc M\xc3\x89LENCHON\n - M. Fran\xc3\xa7ois ASSELINEAU\n - M. Fran\xc3\xa7ois FILLON\n\nThe idea was to collect the data from the Twitter API periodically. The acquisition process evolved as follows: \n\n - Versions 1, 2 and 3\nEvery hour a python script listens to the twitter api stream for 10 minutes during 3 weeks.\n - Version 4+\nThe new versions will be based on a new data structure, and start after the validation by the French constitutional council on 18 March 2017 of the candidates.\n\nThe data will be stored in a dbsqlite files(database_number of the week_number_block_weekday.sqlite format)  and will be updated as often as I can (at least every week).\n\nAfter the first round (version 18+) i had to readjust the number of files per week and the 20 files kaggle limitation push me to reduce the number of files to upload (but you can join for your local analytics the version 17 + version 18+)\n\n**Example : Illustration of the number of mentions of the different candidates**\n![illustration of the mention of the different candidates][4]\n\nI add to these databases a sqlite database that contains the informations from the google trends about the top 5 candidates.In thid database there is :\n\n - A table that contains the overall interests by region\n - A table that contains the interests by region for each candidate\n - A table with the top25 associated queries for each candidate in top and rising ranking\n\n# Content\n\nIn this dbsqlite file, you will find a data table that contains for every row:\n\n===============Common===============\n\n - the index of the line\n - the language of the tweet\n - for each candidate\n:mention_candidatename, if the candidate or his associated account has been called (0 or 1)\n - the tweet\n - the timestamp in milliseconds\n\n===============Version 4+===============\n\n - the day\n - the hour (London timezone)\n - the username of the user that made the tweet\n - the username location (that he gives with his profile)\n - if the tweet is a retweet or a quote (0 or 1)\n - the username that has been retweeted\n - the original tweet (the one retweeted or quoted)\n\n\n# Acknowledgements\nThis election is gonna be intense.\n \n# Inspiration\nThe first version of the dataset was just a test to collect the data and see the first pieces of work  that the community can do with this dataset.The new versions are (I think and hope) adapted to do deep text analytics. \n\n\n  [1]: https://en.wikipedia.org/wiki/French_presidential_election,_2017\n  [2]: https://github.com/jeanmidevacc/french-presidential-election-twitter-pov.git\n  [3]: https://en.wikipedia.org/wiki/French_presidential_election,_2017\n  [4]: https://www.kaggle.io/svf/1039441/a6d419b95420d7c28bd3e9a4437a6370/__results___files/__results___8_2.png\n  [5]: https://trends.google.co.uk/trends/explore?date=today%201-m&geo=FR&q=%2Fm%2F011ncr8c,%2Fm%2F0fqmlm,%2Fm%2F02rdgs,%2Fm%2F04zzm99,%2Fm%2F0551nw\n  [6]: https://www.kaggle.io/svf/1049376/e54172fcff84c97fff6583431eaf9567/__results___files/__results___16_1.png'","b""['politics', 'large', 'featured']""",https://www.kaggle.com/jeanmidev/french-presidential-election
b'H-1B Visa Petitions 2011-2016',b'3 million petitions for H-1B visas',"b""# Context \n\nH-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\n\nThe following articles contain more information about the H-1B visa process:\n\n* [What is H1B LCA ? Why file it ? Salary, Processing times \xe2\x80\x93 DOL](https://redbus2us.com/what-is-h1b-lca-why-file-it-salary-processing-times-dol/)\n* [H1B Application Process: Step by Step Guide](http://www.immi-usa.com/h1b-application-process-step-by-step-guide/)\n\n# Content\n\nThis dataset contains five year's worth of H-1B petition data, with approximately 3 million records overall. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\n\nFor more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in [an official data dictionary](https://www.foreignlaborcert.doleta.gov/docs/Performance_Data/Disclosure/FY15-FY16/H-1B_FY16_Record_Layout.pdf).\n\n# Acknowledgements\n\nThe Office of Foreign Labor Certification (OFLC) generates program data, including data about H1-B visas. The disclosure data updated annually and is available [online](https://www.foreignlaborcert.doleta.gov/performancedata.cfm).\n\nThe raw data available is messy and not immediately suitable analysis. A set of data transformations were performed making the data more accessible for quick exploration. To learn more, refer to [this blog post](https://sharan-naribole.github.io/2017/02/24/h1b-eda-part-I.html) and to the complimentary [R Notebook](https://github.com/sharan-naribole/H1B_visa_eda/blob/master/data_processing.Rmd).\n\n# Inspiration\n\n* Is the number of petitions with Data Engineer job title increasing over time?\n* Which part of the US has the most Hardware Engineer jobs?\n* Which industry has the most number of Data Scientist positions?\n* Which employers file the most petitions each year?""","b""['law', 'international relations', 'medium', 'featured']""",https://www.kaggle.com/nsharan/h-1b-visa
b'Mathematicians of Wikipedia',"b""A Dataset of the World's Most Famous Mathematicians""","b'### Context\n\nWhat distinguishes the great from the good, the remembered from the accomplished, and the genius from the merely brilliant? Scrapping English Wikipedia, Joseph Philleo has cleaned and compiled a database of more than 8,500 famous mathematicians for the Kaggle data science community to analyze and better understand.\n\n### Inspiration\n\n - What are the common characteristics of famous mathematicians?\n - How old do they live, which fields do they work in, where are they born, and where do they live?\n - Can you predict which mathematicians will win a Fields Medal, join the Royal Society, or secure tenure at Harvard?'","b""['internet', 'demographics', 'mathematics', 'people', 'medium', 'featured']""",https://www.kaggle.com/joephilleo/mathematicians-on-wikipedia
b'New York City - Certificates of Occupancy',b'New and newly reconstructed buildings in New York City.',"b'### Context: \nThe City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. \xe2\x80\x9cgut renovated\xe2\x80\x9d) buildings in New York City. These documents assert that the city has deemed the building habitable and safe to move into.\n\n### Content: \nThis dataset includes all temporary (expirable) and final (permanent) Certificates of Occupancies issues to newly habitable buildings in New York City, split between new (Job Type: NB) and reconstructed (Job Type: A1) buildings, issued between July 12, 2012 and August 29, 2017.\n\n### Acknowledgements: \nThis data is published as-is by the New York City Department of Buildings.\n\n### Inspiration: \n* In what areas of New York City are the newly constructed buildings concentrated?\n* What is the difference in distribution between buildings that are newly built and ones that are newly rebuilt?\n* In combination with the [New York City Buildings Database](https://www.kaggle.com/new-york-city/nyc-buildings/) dataset, what are notable differences in physical characteristics between recently constructed buildings and existing ones?'","b""['cities', 'civil engineering', 'medium', 'featured']""",https://www.kaggle.com/new-york-city/nyc-certificates-of-occupancy
b'Who starts and who debunks rumors',b'Webpages cited by rumor trackers',"b'# Context \n\n[Emergent.info](http://www.emergent.info/) was a major rumor tracker, created by veteran journalist [Craig Silverman](https://twitter.com/CraigSilverman). It has been defunct for a while, but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web. \n\n[Snopes.com](http://www.snopes.com/) is one of the oldest rumors trackers on the web. Originally launched by Barbara and David Mikkelson, it is now run by a team of editors who investigate urban legends, myths, viral rumors and fake news. The investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor, often citing several web pages and other external sources. \n\n[Politifact.com](http://www.politifact.com/) is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns, blogs and similar websites. Politifact\'s labels range from ""true,"" to ""pants on fire!""  \n\n---\n\n# Content\n\nThis dataset consists of three files. One file is a collection of all webpages cited in Emergent.info, and the second is a collection of webpages cited in Snopes.com, and the third is a similar collection from Politifact.com. The webpages were often cited because they had started a rumor, shared a rumor, or debunked a rumor. \n\n###Emergent.info\nEmergent.info often provides a clean timeline of the rumor\'s propagation on the web, and identifies which page was for the rumor, which page was against it, and which page was simply observing it. Please refer to the image below to learn more about the fields in this dataset.\n\n![The image displays a sample post from Emergent.info and highlights the corresponding fields in emergent.csv.][1]\n\n###Snopes.com\nThe structure of posts on **Snopes.com** is not as well-defined. Please refer to the image below to learn more about the fields in the Snopes dataset.\n\n![This image displays a sample post from Snopes.com and highlights the corresponding fields in snopes.csv.][2]\n\n###Politifact.com\nSimilar to Emergent.info, Politifact.com follows a well-structured format in reporting and documenting rumors. There is a sidebar on the right side of each page that lists all of the sources cited within the page. The top link is the likeliest to be the original source of the rumor. For this link, page_is_first_citation is set to true. \n\n![This image displays a sample post from Politifact.com and highlights the corresponding fields in politifact.csv.][3]\n\n---\n\n# Inspiration\n\nI created this dataset in order to study domains that frequently start, propagate, or debunk rumors. By studying these domains and people who follow them, I hope to gain some insight into the dynamics of rumor propagation on the web, as well as social media. \n\n---\n\n# Notes/Disclaimer\n\nWhen using the Snopes dataset, please keep the following in mind: \n\n* In addition to debunking rumors, Snopes.com occasionally reports news and other types of content. This collection only includes data from ""[Fact Check](http://www.snopes.com/category/facts)"" posts on Snopes.\n \n* Snopes.com was launched years ago. Some of the older posts on the website do not follow the current format of the site, therefore some of the fields might be missing.\n\n* Snopes.com used to use a service named ""[DoNotLink.com](https://twitter.com/donotlink?lang=en)"" for citation purposes. That service is no longer active and as a result some of the links are missing from older posts on Snopes. \n\n* In addition, some of the shortened links would time-out prior to resolution, in which case they would not be added to the dataset. \n\n* Occasionally, a website that has been cited has not maliciously started a rumor. For instance, Andy Borowitz is a humorist who writes for *The New Yorker*. His satirical column is sometimes mistaken for real news; as a result, *The New Yorker* may be cited as a source of fake news on [Snopes.com](http://www.snopes.com/trump-blasts-media-for-reporting-things-he-says/). This does not mean that *The New Yorker* is a fake news website.\n\nWhen using the Politifact dataset, please keep the following in mind:\n\n* The data included in this dataset are collected from the ""[truth-o-meter](http://www.politifact.com/punditfact/statements/)"" page of Politifact.com.\n\n* Politifact often fact-checks statements made by politicians. Since this dataset is focused on websites, I have ignored all the posts in which the rumor was attributed to a person, a political party, a campaign, or an organization. Instead, I have only included rumors attributed explicitly to websites or blogs. \n\n---\n\n# Useful Tips for Using the Snopes collection\n\nAs opposed to the Emergent collection where each page is flagged with whether it was for or against a rumor, no such information is available for the Snopes dataset. To avoid manually labeling the data, you may use the following heuristics to identify which page started a rumor:\n\n* Webpages that are cited in the ""Examples"" section of a post are often ""observing"" the rumor, i.e. they have not started it, but they are repeating it. In the snopes.csv file, these webpages have been flagged as ""page_is_example.""\n\n* Webpages that are cited in the ""Featured Image"" section of a post are often not related to the rumor. The editors on Snopes have simply extracted an image from those pages to embed in their posts. In the snopes.csv file, these webpages have been flagged as ""page_is_image_credit.""\n\n* Webpages that are cited through a secondary service (such as [archive.is](http://archive.is/)) are likelier to be rumor-propagators. Editors do not link to them directly so that a record of their page is available, even if it is later deleted.\n\n* If neither of these hints help, very often (but not always) the first link cited on the page (for which ""page_is_example"" and ""page_is_image_credit"" are false) is the link to a page that started the rumor. This link is identified by the ""page_is_first_citation"" field. Pages for which both ""page_is_first_citation"" and ""page_is_archived"" are true are very likely to be rumor propagators. \n\n* To identify satirical websites that are mistaken for real news, it\'s useful to inspect the way they are cited on Snopes. To demonstrate that a website contains satire or humor, Snopes writers often cite the ""about us"" page of the site. Therefore it\'s useful to  see which domains often contain a URI to their ""about"" page (e.g. ""http://politicops.com/about-us/"").\n\n  [1]: http://imgur.com/JZPExar.png\n  [2]: http://i.imgur.com/jFT6Vdb.png\n  [3]: http://i.imgur.com/Z83JP7c.png'","b""['linguistics', 'sociology', 'small', 'featured']""",https://www.kaggle.com/arminehn/rumor-citation
b'Cuneiform Digital Library Initiative',b'Explore thousands of ancient tablet transliterations',"b'What is CDLI?\n--------------------\nThe Cuneiform Digital Library Initiative (CDLI) is an international digital library project aimed at putting text and images of an estimated 500,000 recovered cuneiform tablets created from between roughly 3350 BC and the end of the pre-Christian era online. The initiative is a joint project of the University of California, Los Angeles, the University of Oxford, and the Max Planck Institute for the History of Science, Berlin. \n\nThis dataset includes the full CDLI catalogue (metadata), transliterations of tablets in the catalogue, and word/sign lists from old akkadian and Ur III. This data was downloaded on the 9th of May 2017.\n\nTransliterations are in .atf format, find out more about this format here: [http://oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html][1]\n\nFind more about CDLI here: [http://cdli.ucla.edu/][2]\n\nWhat is Cuneiform?\n--------------------\nCuneiform script, one of the earliest systems of writing, was invented by the Sumerians. It is distinguished by its wedge-shaped marks on clay tablets, made by means of a blunt reed for a stylus. The name cuneiform itself simply means ""wedge shaped"".\n\nCuneiform is not a language, nor is it an alphabet. Cuneiform uses between 600-1000 characters to write words or syllables. It has been used by many different cultural groups to represent many different languages, but it was primarily used to write Sumerian and Akkadian. Deciphering cuneiform is very difficult to this day, though the difficulty varies depending on the language.\n\n[https://en.wikipedia.org/wiki/Cuneiform_script][3]\n\nWhat is Assyriology?\n--------------------\nAssyriology is the study of the languages, history, and culture of the people who used the ancient writing system called cuneiform. Cuneiform was used primarily in an area called the Near East, centred on Mesopotamia (modern Iraq and eastern Syria) where cuneiform was invented, but including the Northern Levant (Western Syria and Lebanon), parts of Anatolia, and western Iran. The sources for Assyriology are all archaeological, and include both inscribed and uninscribed objects. Most Assyriologists focus on the rich textual record from the ancient Near East, and specialise in either the study of language, literature, or history of the ancient Near East.\n\nAssyriology began as an academic discipline with the recovery of the monuments of ancient Assyria, and the decipherment of cuneiform, in the middle of the 19th century. Large numbers of archaeological objects, including texts, were brought to museums in Europe and later the US, following the early excavations of Nineveh, Kalhu, Babylon, Girsu, Assur and so forth. Today Assyriology is studied in universities across the globe, both as an undergraduate and a graduate subject, and knowledge from the ancient Near East informs students of numerous other disciplines such as the History of Science, Archaeology, Classics, Biblical studies and more.\n\n\n  [1]: http://oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html\n  [2]: http://cdli.ucla.edu/\n  [3]: https://en.wikipedia.org/wiki/Cuneiform_script'","b""['linguistics', 'languages', 'history', 'medium', 'featured']""",https://www.kaggle.com/mylesoneill/cuneiform-digital-library-initiative
b'State of the Nation Corpus (1990 - 2018)',b'Full texts of the South African State of the Nation addresses',"b'### Context\n\nThe [State of the Nation Address][1] of the President of South Africa (abbreviated SONA) is an annual event in the Republic of South Africa, in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament (the National Assembly and the National Council of Provinces).\n\n### Content\n\nFull text of all the speeches, from 1990 through to 2018.  In years that elections took place, a State of the Nation Address happens twice, once before and again after the election.\n\n  [1]: https://en.wikipedia.org/wiki/State_of_the_Nation_Address_(South_Africa)'","b""['small', 'featured']""",https://www.kaggle.com/allank/state-of-the-nation-1990-2017
"b'The ""Trump Effect"" in Europe'",b'A post-election survey about populism in the US and the EU-28',"b'**Context** \n\nThe election of Donald Trump has taken the world by surprise and is fuelling populist movements in Europe, e.g. in Italy, Austria and France. Understanding populism and assessing the impact of the \xe2\x80\x9cTrump effect\xe2\x80\x9d on Europe is a tremendous challenge, and Dalia wants to help pool brainpower to find answers. \n\nThe goal is to find out where the next wave of populism could hit in Europe by comparing and contrasting US and EU voter profiles, opinions of Trump vs Clinton voters, Brexit vs. Bremain voters, and future expectations.\n\n**Content**\n\nExpanding Dalia\xe2\x80\x99s quarterly ""[EuroPulse][1]"" omnibus survey to the USA, Dalia has conducted a representative survey with n=11.283 respondents across all 28 EU member countries and n=1.052 respondents from the United States of America. To find out where the next wave of populism could hit Europe, Dalia\xe2\x80\x99s survey traces commonalities in social and political mindsets (like authoritarianism, prejudice, open-mindedness, xenophobia, etc.), voting behaviour and socio-demographic profiles on both sides of the Atlantic.\n\n**Inspiration**\n\nThe sources of our inspirations are many, but to name a few who influenced the way we asked questions: we were very inspired by the \'angry voter\' profile laid out by [Douglas Rivers][3], the influence of political and moral attitudes pointed out by [Jonathan Haidt][4] and the profile of ""America\'s forgotten working class"" by [J. D. Vance][5]. \n\n**Researchers should apply the necessary logic, caution and diligence when analysing and interpreting the results.** \n\n\n  [1]: https://daliaresearch.com/europulse/\n  [2]: https://daliaresearch.com/wp-content/uploads/2016/08/Methodology-PDF-1.pdf\n  [3]: http://www.americanacademy.de/home/person/douglas-rivers\n  [4]: https://www.ted.com/speakers/jonathan_haidt\n  [5]: https://www.ted.com/speakers/jd_vance\n  [6]: https://fivethirtyeight.com/'","b""['politics', 'international relations', 'medium', 'featured']""",https://www.kaggle.com/daliaresearch/trump-effect
b'Independent Political Ad Spending (2004-2016)',b'Spending on political ads by independent (non-candidate) groups',"b'## What is an Independent Expenditure?\n\nIndependent expenditures are what some refer to as ""hard money"" in politics -- spending on ads that specifically mention a candidate (either supporting or opposing). The money for these ads must come from PACs that are independent of the candidate and campaign, and the PACs cannot coordinate with the candidate.\n\nThe Federal Election Commission (FEC) collects information on independent expenditures to ensure payers\' independence from candidates.\n\n## What can we look at?\n\nI\'m super interested to see how much spending has increased over the years. The FEC data only goes back to 2004, and it may be the case that the older data is spotty, but I don\'t doubt that political spending has gone up in the past few years (the 2016 Presidential campaign reportedly involved the most political money since the 1970s).\n\n## What does the data look like?\n\nThis dataset includes a ton of information from the independent expenditure reports:\n\n- **committee_id** : unique id of the PAC that made the payment\n- **committee_name** : name of the PAC that made the payment\n- **report_year** : the year the report was file\n- **report_type** : one of 24 or 48; whether this is a 24-hour report or a 48-hour report\n- **image_number** : unique id of the scanned image of the report\n- **line_number** : line number in the report\n- **file_number** : unique id of the report\n- **payee_name** : who got paid\n- **payee_first_name** : if an individual payee, their first name\n- **payee_middle_name** : if an individual payee, their middle name\n- **payee_last_name** : if an individual payee, their last name\n- **payee_street_1** : payee street address (1 of 2)\n- **payee_street_2** : payee street address (2 of 2)\n- **payee_city** : payee city\n- **payee_state** : payee state\n- **payee_zip** : payee ZIP code\n- **expenditure_description** : a string describing the expenditure\n- **expenditure_date** : when was this expenditure made?\n- **dissemination_date** : when was the advertisement disseminated?\n- **expenditure_amount** : how much was spent?\n- **office_total_ytd** : how much has this PAC spent on this office, year-to-date?\n- **category_code** : category of the expenditure (*need to find categories!*)\n- **category_code_full** : category of the expenditure (*need to find categories!*)\n- **support_oppose_indicator** : one of S or O; whether the ad is in support of or opposition to the candidate\n- **memo_code** :\n- **memo_code_full** :\n- **candidate_id** : unique id of the candidate\n- **candidate_name** : name of the candidate\n- **candidate_prefix** : title or prefix of the candidate\n- **candidate_first_name** : first name of the candidate\n- **candidate_middle_name** : middle name of the candidate\n- **candidate_last_name** : last name of the candidate\n- **candidate_suffix** : suffix of the candidate\'s name\n- **candidate_office** : office that the candidate is running for -- one of P (President), S (Senate), or H (House)\n- **cand_office_state** : if House or Senate race, in what state?\n- **cand_office_district** : if House or Senate race, in what district?\n- **conduit_committee_id** :\n- **conduit_committee_name** :\n- **conduit_committee_street1** :\n- **conduit_committee_street2** :\n- **conduit_committee_city** :\n- **conduit_committee_state** :\n- **conduit_committee_zip** :\n- **election_type** : one of P (primary) or G (general)\n- **election_type_full** : an id comprising the election type and the year, with no delimiter\n- **independent_sign_name** :\n- **independent_sign_date** :\n- **notary_sign_name** :\n- **notary_sign_date** :\n- **notary_commission_expiration_date** :\n- **back_reference_transaction_id** :\n- **back_reference_schedule_name** :\n- **filer_first_name** : \n- **filer_middle_name** :\n- **filer_last_name** :\n- **transaction_id** : unique id identifying the transaction\n- **original_sub_id** :\n- **action_code** :\n- **action_code_full** :\n- **schedule_type_full** :\n- **filing_form** :\n- **link_id** :\n- **sub_id** :\n- **payee_prefix** :\n- **payee_suffix** :\n- **is_notice** :\n- **memo_text** :\n- **filer_prefix** :\n- **filer_suffix** :\n- **schedule_type** :\n- **pdf_url** : link to the scanned form'","b""['finance', 'politics', 'medium', 'featured']""",https://www.kaggle.com/fec/independent-political-ad-spending
b'Foreign Direct Investment in India',b'Sector & Financial year wise time series data from 2000-2016.',b'**Connect/Follow me on [LinkedIn](http://link.rajanand.org/linkedin) for more updates on interesting dataset like this. Thanks.**\n\n### Context\n\nTo understand the Foreign direct investment in India for the last 17 years from 2000-01 to 2016-17.\n\n### Content\n\nThis dataset contains sector and financial year wise data of FDI in India.\n\n### Acknowledgements\n\n[Ministry of Commerce and Industry](http://commerce.gov.in/EIDB.aspx) has published Financial Year wise FDI Equity Inflows from 2000-01 to 2016-17 dataset in [Open Government Data Platform India](https://data.gov.in/catalog/foreign-direct-investment-fdi-equity-inflows) under [Govt. Open Data License - India](https://data.gov.in/government-open-data-license-india).\n\n### Inspiration\n  \n  \n  - How much FDI has changed over the year?\n  - How much has varied since 2014 after Narendra Modi become PM of India?',"b""['finance', 'india', 'small', 'featured']""",https://www.kaggle.com/rajanand/fdi-in-india
b'Synchronized Brainwave Dataset',b'Brainwave recordings from a group presented with a shared audio-visual stimulus',"b'### Context\n\nEEG devices are becoming cheaper and more inconspicuous, but few applications leverage EEG data effectively, in part because there are few large repositories of EEG data. The MIDS class at the UC Berkeley School of Information is sharing a dataset collected using consumer-grade brainwave-sensing headsets, along with the software code and visual stimulus used to collect the data. The dataset includes all subjects\' readings during the stimulus presentation, as well as readings from before the start and after the end of the stimulus.\n\n### Content\n\nWe presented two slightly different stimuli to two different groups. [Stimuli 1 is available here](https://youtu.be/zkGoPdpRvaU), and [stimuli 2 is available here](https://youtu.be/sxqlOoBBjvc). \n\nFor both stimuli, a group of about 15 people saw the stimuli at the same time, while EEG data was being collected. The stimuli each person saw is available in the `session` field of `subject-metadata.csv`. (Subjects who saw stimulus 2 left the room during stimulus 1, and vice versa).\n\nFind the synchronized times for both stimuli in `stimulus-timing.csv`.\n\nFor each participant, we also anonymously collected some other metadata: (1) whether or not they had previously seen the video displayed during the stimulus (a superbowl ad), (2) gender, (3) whether or not they saw hidden icons displayed during the color counting exercise, and (4) their chosen color during the color counting exercise. All of these can be found in `subject-metadata.csv`.\n\nWe also collected the timing (in `indra_time`) of all stimulus events for both session 1 and session 2. These times are included in `stimulus-times.csv`.\n\nThe server receives one data packet every second from each Mindwave Mobile device, and stores the data in one row entry.\n\n### Acknowledgements\n\nPlease use the following citation if you publish your research results using this dataset or software code or stimulus file:\n\n**John Chuang, Nick Merrill, Thomas Maillart, and Students of the UC Berkeley Spring 2015 MIDS Immersion Class. ""Synchronized Brainwave Recordings from a Group Presented with a Common Audio-Visual Stimulus (May 9, 2015)."" May 2015.**'","b""['healthcare', 'human-computer interaction', 'medium', 'featured']""",https://www.kaggle.com/berkeley-biosense/synchronized-brainwave-dataset
b'United States Droughts by County',b'Weekly data on extent and severity of drought in each US county (2000-present)',"b'The [United States Drought Monitor](http://droughtmonitor.unl.edu/Home.aspx) collects weekly data on drought conditions around the U.S.\n\n## Acknowledgements\n\nAll data was downloaded from the [United States Drought Monitor webpage](http://droughtmonitor.unl.edu/Home.aspx).\n\nThe U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC-UNL.\n\n## The Data\n\nThe data contains weekly observations about the extent and severity of drought in each county of the United States. The dataset contains the following fields:\n\n- **releaseDate**: when this data was released on the USDM website\n- **FIPS**: the FIPS code for this county\n- **county**: the county name\n- **state**: the state the county is in\n- **NONE**: percentage of the county that is *not in drought*\n- **D0**: percentage of the county that is in *abnormally dry conditions*\n- **D1**: percentage of the county that is in *moderate drought*\n- **D2**: percentage of the county that is in *severe drought*\n- **D3**: percentage of the county that is in *extreme drought*\n- **D4**: percentage of the county that is in *exceptional drought*\n- **validStart**: the starting date of the week that these observations represent\n- **validEnd**: the ending date of the week that these observations represent\n- **domStatisticFormatID**: seems to always be 1\n\n**Note:** the drought categories are cumulative: if an area is in D3, then it is also in D2, D1, and D0. This means that, for every observation, D4 <= D3 <= D2 <= D1 <= D0.\n\n### County Info\n\nTo make some analyses slightly easier, I\'ve also included *county_info_2016.csv*, which contains physical size information about each county. This file contains the following fields:\n\n- **USPS**: \tUnited States Postal Service State Abbreviation\n- **GEOID**: \tFIPS code\n- **ANSICODE**: \tAmerican National Standards Institute code\n- **NAME**: \tName\n- **ALAND**: \tLand Area (square meters) - Created for statistical purposes only\n- **AWATER**: \tWater Area (square meters) - Created for statistical purposes only\n- **ALAND_SQMI**: \tLand Area (square miles) - Created for statistical purposes only\n- **AWATER_SQMI**: \tWater Area (square miles) - Created for statistical purposes only\n- **INTPTLAT**: \tLatitude (decimal degrees) First character is blank or ""-"" denoting North or South latitude respectively\n- **INTPTLONG**: \tLongitude (decimal degrees) First character is blank or ""-"" denoting East or West longitude respectively'","b""['climate', 'geography', 'medium', 'featured']""",https://www.kaggle.com/us-drought-monitor/united-states-droughts-by-county
b'When do children learn words?',b'How common 731 Norwegian words are & when children learn them',"b'### Context: \nWhen children are born they don\xe2\x80\x99t know any words. By the time they\xe2\x80\x99re three, most children know 200 words or more. These words aren\xe2\x80\x99t randomly selected from the language they\xe2\x80\x99re learning, however. A two-year-old is much more likely to know the word \xe2\x80\x9cbottle\xe2\x80\x9d than the word \xe2\x80\x9ctitration\xe2\x80\x9d. What words do children learn first, and what qualities do those words have? This dataset was collected to explore this question.\n\n### Content: \nThe main dataset includes information for 732 Norwegian words. A second table also includes measures of how frequently each word is used in Norwegian, both on the internet (as observed in the [Norwegian Web as Corpus dataset](http://www.hf.uio.no/iln/om/organisasjon/tekstlab/prosjekter/nowac/)) and when an adult is talking to a child. The latter is commonly called \xe2\x80\x9cchild directed speech\xe2\x80\x9d and is abbreviated as \xe2\x80\x9cCDS\xe2\x80\x9d.\t\t\t\n\n#### Main data\n\n* ID_CDI_I: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 1\n* ID_CDI_II: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 2\n* Word_NW: The word in Norwegian\n* Word_CDI: The form of the word found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories\n* Translation: the English translation of the Norwegian word\n* AoA: how old a child generally is was when they this this word, in months (Estimated from the MacArthur-Bates Communicative Development Inventories)\n* VSoA: how many other words a child generally knows when they learn this word (rounded up to the nearest 10)\n* Lex_cat: the specific part of speech of the word\n* Broad_lex: the broad part of speech of the word\n* Freq: a measure of how commonly this word occurs in Norwegian\n* CDS_Freq: a measure of how commonly this word occurs when a Norwegian adult is talking to a Norwegian child\n\n#### Norwegian CDS Frequency\n\n* Word_CDI: The word from, as found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories\n* Translation: The English translation of the Norwegian word\n* Freq_NoWaC: How often this word is used on the internet\n* Freq_CDS: How often this word is used when talking to children (based on two Norwegian [CHILDES corpora](http://childes.talkbank.org/))\n\n### Acknowledgements: \nThis dataset was collected by Pernille Hansen. If you use this data, please cite the following paper: \n\nHansen (2016). What makes a word easy to acquire? The effects of word class, frequency, imageability and phonological neighbourhood density on lexical development. First Language. Advance online publication. doi: 10.1177/0142 723716679956 http://dx.doi.org/10.1177/0142723716679956 \n\n### Inspiration: \n\n* How well can you predict which words a child will learn first?\n* Are some sounds or letters found more often than chance in words learned early?\n* Can you build topic models on earlier-acquired and later-acquired words? Which topics are over-represented in words learned very early?'","b""['linguistics', 'europe', 'languages', 'small', 'featured']""",https://www.kaggle.com/rtatman/when-do-children-learn-words
b'StarCraft II matches history',b'Predict the results of matches in StarCraft 2 using historical data',"b'### Context\n\nThis data set is a collection of all StarCraft pro-player 2 matches. The data is taken from the site - [http://aligulac.com/][1]\n\n### Content\n\nDataset data - 18 October 2017. You can parse actual data. Just use my script ([Github][2])\n\nThis dataset contains 10 variables:\n\n 1. **match_date** -Date of match in format mm/dd/yyyy\n 2. **player_1** - Player 1 Nickname\n 3. **player_1_match_status** - Match status for Player 1: winner or loser\n 4. **score** - match score (example: 1-0, 1-2 etc)\n 5. **player_2** - Player 2 Nickname\n 6. **player_2_match_status** - Match status for Player 2: winner or loser\n 7. **player_1_race** - Player 1 Race: Z - Zerg, P - Protoss, T - Terran\n 8. **player_2_race** - Player 2 Race: Z - Zerg, P - Protoss, T - Terran\n 9. **addon** - Game addon: WoL- Wings of Liberty, HotS - Heart of the Swarm, LotV - Legacy of the Void\n 10. **tournament_type** - online or offline\n\n### Acknowledgements\n\nThe source is [http://aligulac.com/][1]\n\n### Inspiration\n\nQuestions worth exploring:\n\n 1. Predict the outcome of a match between two players\n 2. or whatever you want ....\n\n  [1]: http://aligulac.com/\n  [2]:https://github.com/alimbekovKZ/starcraft2-matches-history-predict'","b""['video games', 'history', 'games and toys', 'strategy', 'medium', 'featured']""",https://www.kaggle.com/alimbekovkz/starcraft-ii-matches-history
"b'UN General Assembly Votes, 1946-2015'",b'Votes by member states on individual resolutions and specific issues',"b'# Content\n\nThis dataset documents all United Nations General Assembly votes since its establishment in 1946. The data is broken into three different files: the first lists each UN resolution, subject, and vote records; the second records individual member state votes per resolution; and the third provides an annual summary of member state voting records with affinity scores and an ideal point estimate in relation to the United States.\n\n\n# Acknowledgements\n\nThe UN General Assembly voting data was compiled and published by Professor Erik Voeten of Georgetown University.'","b""['politics', 'international relations', 'medium', 'featured']""",https://www.kaggle.com/unitednations/general-assembly
b'The General Social Survey (GSS)',"b'Longitudinal study of popular beliefs, attitudes, morality & behaviors in the US'","b'\xe2\x80\x8b\xe2\x80\x8bThe GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes.  Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.\n\nThe GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\n\nAltogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. ([Source][1])\n\nThis dataset is a csv version of the [Cumulative Data File][2], a cross-sectional sample of the GSS from 1972-current.\n\n  [1]: http://gss.norc.org/About-The-GSS\n  [2]: http://gss.norc.org/get-the-data/spss'","b""['ethics', 'political science', 'large', 'featured']""",https://www.kaggle.com/norc/general-social-survey
"b'Museums, Aquariums, and Zoos'","b'Name, location, and revenue for every museum in the United States'","b'# Content\n\nThe museum dataset is an evolving list of museums and related organizations in the United States. The data file includes basic information about each organization (name, address, phone, website, and revenue) plus the museum type or discipline. The discipline type is based on the National Taxonomy of Exempt Entities, which the National Center for Charitable Statistics and IRS use to classify nonprofit organizations.\n\nNon-museum organizations may be included. For example, a non-museum organization may be included in the data file because it has a museum-like name on its IRS record for tax-exempt organizations. Museum foundations may also be included.\n\nMuseums may be missing. For example, local municipal museums may be undercounted because original data sources used to create the compilation did not include them.\n\nMuseums may be listed multiple times. For example, one museum may be listed as both itself and its parent organization because it was listed differently in each original data sources. Duplicate records are especially common for museums located within universities.\n\nInformation about museums may be outdated.  The original scan and compilation of data sources occurred in 2014.  Scans are no longer being done to update the data sources or add new data sources to the compilation.  Information about museums may have changed since it was originally included in the file.\n\n\n# Acknowledgements\n\nThe museum data was compiled from IMLS administrative records for discretionary grant recipients, IRS records for tax-exempt organizations, and private foundation grant recipients.\n\n\n# Inspiration\n\nWhich city or state has the most museums per capita? How many zoos or aquariums exist in the United States? What museum or related organization had the highest revenue last year? How does the composition of museum types differ across the country?'","b""['animals', 'museums', 'small', 'featured']""",https://www.kaggle.com/imls/museum-directory
b'National Wetlands Inventory',b'Location and Type of Wetlands and Deepwater Habitats in the United States',"b'## Context\n\nThe data delineate the areal extent of wetlands and surface waters as defined by Cowardin et al. (1979). Certain wetland habitats are excluded from the National mapping program because of the limitations of aerial imagery as the primary data source used to detect wetlands. These habitats include seagrasses or submerged aquatic vegetation that are found in the intertidal and subtidal zones of estuaries and near shore coastal waters. Some deepwater reef communities (coral or tuberficid worm reefs) have also been excluded from the inventory. These habitats, because of their depth, go undetected by aerial imagery. By policy, the Service also excludes certain types of ""farmed wetlands"" as may be defined by the Food Security Act or that do not coincide with the Cowardin et al. definition. Contact the Service\'s Regional Wetland Coordinator for additional information on what types of farmed wetlands are included on wetland maps. \n\n## Content\n\nThe dataset includes:\n\n* **OBJECTID**\n\n* **ATTRIBUTE**\n\n* **WETLAND_TYPE**\n\n* **ACRES**\n\n* **GLOBALID**\n\n* **ShapeSTArea**\n\n* **ShapeSTLength**\n\n## Acknowledgement\n\nThe original dataset and metadata can be found [here](http://opendata.arcgis.com/datasets/a86598be317040febc67941ac7a2768e_18).\n\n## Inspiration\n\n* Can you visualizes the differences in the wetlands shape by type?'","b""['utility', 'ecology', 'medium', 'featured']""",https://www.kaggle.com/arcgisopendata/national-wetlands-inventory
b'What is a note?',b'A collection of notes played on a guitar',"b'**Overview**\n\nWe could take a music theory class to understand *what* a note is, but why don\'t we just find out for ourselves? In this data set we have the notes on a guitar on open strings, and on the 1st-8th frets on every string. The notes were recorded on a nice but low-end guitar called the Yamaha C-40.  The guitar is in standard tuning (from the top string to the bottom on we have E low, A, D, G, B, E high).\n\n**What to do with this dataset**\n\nI didn\'t label any notes (except the open ones - an open A string is an A). If you want a challenge you can cluster the notes and see if your clustering lumps all the same notes together. If you want labels so you can do some inspection of notes that are the same you can look on google for a guitar fretboard diagram. I have done both of these experiments and learned a bit about music which I \'m hoping to verify soon when I find a good music theory book.\n\nThis data set might be interesting to use to be able to write sheet music from an audio sample of some finger-picked music. Identifying chords is a difficult computational task, but finger-style guitar, with clear, individual notes, might be easier. If this is the case, a simple script could be written to write sheet music from an audio sample.\n\nOne draw back about the data set is that some non-plucked strings were vibrating when I played a note. I tried various techniques to muffle them, but there is still some noise in the background. I don\'t know if this is because of my technique or something that happens to all players on all guitars. In any event, this noise didn\'t hurt my analysis.\n\n**About the data**\n\nThey were recorded by me, Melvyn, using a program called audacity.\n\nThere is a directory with the name of the string. Inside the directory you will find .wav files named either open, 1, 2, ....8 for the fingering of the string. There is also a directory called ""scale"" I recorded some notes that make a ""do-re-mi..."" scale. You can use these for a number of things.\n\nI use the GuitarTuner app to tune the Guitar - I\'m just learning so I don\'t have an ear for notes yet. After some initial analysis it looks like the guitar might be a bit out of tune, so the resonant frequencies are a bit off from what they should be. Another thing that is interesting to think about it is how far a frequency must be from the proper on until it becomes distinguishable as a different note.'","b""['music', 'medium', 'featured']""",https://www.kaggle.com/juliancienfuegos/what-is-a-note
b'Human Mobility During Natural Disasters',b'Twitter geolocation for users during 15 natural disasters',"b""This dataset contains geolocation information for thousands of Twitter users during natural disasters in their area. \n\n### Abstract\n**(from original paper)**\n\nNatural disasters pose serious threats to large urban areas, therefore understanding and predicting human movements is critical for evaluating a population\xe2\x80\x99s vulnerability and resilience and developing plans for disaster evacuation, response and relief. However, only limited research has been conducted into the effect of natural disasters on human mobility. This study examines how natural disasters influence human mobility patterns in urban populations using individuals\xe2\x80\x99 movement data collected from Twitter. We selected fifteen destructive cases across five types of natural disaster and analyzed the human movement data before, during, and after each event, comparing the perturbed and steady state movement data. The results suggest that the power-law can describe human mobility in most cases and that human mobility patterns observed in steady states are often correlated with those in perturbed states, highlighting their inherent resilience. However, the quantitative analysis shows that this resilience has its limits and can fail in more powerful natural disasters. The findings from this study will deepen our understanding of the interaction between urban dwellers and civil infrastructure, improve our ability to predict human movement patterns during natural disasters, and facilitate contingency planning by policymakers.\n\n## Acknowledgments\nThe original journal article for which this dataset was collected:  \nWang Q, Taylor JE (2016) Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. PLoS ONE 11(1): e0147299. [http://dx.doi.org/10.1371/journal.pone.0147299](http://dx.doi.org/10.1371/journal.pone.0147299)\n\nThe Dryad page that this dataset was downloaded from:  \nWang Q, Taylor JE (2016) Data from: Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. Dryad Digital Repository. [http://dx.doi.org/10.5061/dryad.88354](http://dx.doi.org/10.5061/dryad.88354)\n\n## The Data\n\nThis dataset contains the following fields:\n\n- **disaster.event**: the natural disaster during which the observation was collected. One of:  \n-- one of:  \n--- *01_Wipha*, *02_Halong*, *03_Kalmaegi*, *04_Rammasun_Manila* (typhoons)  \n--- *11_Bohol*, *12_Iquique*, *13_Napa* (earthquakes)  \n--- *21_Norfolk*, *22_Hamburg*, *23_Atlanta* (winter storms)  \n--- *31_Phoenix*, *32_Detroit*, *33_Baltimore* (thunderstorms)  \n--- *41_AuFire1*, *42_AuFire2* (wildfires)  \n- **user.anon**: an anonymous user id; unique within each disaster event  \n- **latitude**: latitude of user's tweet  \n- **longitude.anon**: longitude of user's tweet; shifted to preserve anonymity\n- **time**: the date and time of the tweet""","b""['internet', 'demographics', 'geography', 'medium', 'featured']""",https://www.kaggle.com/dryad/human-mobility-during-natural-disasters
b'Flight Route Database',"b'A database of 59,036 flight routes'","b'### Routes database\n\nAs of January 2012, the OpenFlights/Airline Route Mapper Route Database contains 59036 routes between 3209 airports on 531 airlines spanning the globe.\n\n### Content\n**The data is ISO 8859-1 (Latin-1) encoded.**\n\nEach entry contains the following information:\n\n- Airline\t2-letter (IATA) or 3-letter (ICAO) code of the airline.\n- Airline ID\tUnique OpenFlights identifier for airline (see Airline).\n- Source airport\t3-letter (IATA) or 4-letter (ICAO) code of the source airport.\n- Source airport ID\tUnique OpenFlights identifier for source airport (see Airport)\n- Destination airport\t3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\n- Destination airport ID\tUnique OpenFlights identifier for destination airport (see Airport)\n- Codeshare\t""Y"" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\n- Stops\tNumber of stops on this flight (""0"" for direct)\n- Equipment\t3-letter codes for plane type(s) generally used on this flight, separated by spaces\n\nThe special value \\N is used for ""NULL"" to indicate that no value is available. \n\nNotes:\n\n- Routes are directional: if an airline operates services from A to B and from B to A, both A-B and B-A are listed separately.\n- Routes where one carrier operates both its own and codeshare flights are listed only once.\n\n### Acknowledgements\n\nThis dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!'","b""['small', 'featured']""",https://www.kaggle.com/open-flights/flight-route-database
b'Video Game Sales with Ratings',b'Video game sales from Vgchartz and corresponding ratings from Metacritic',"b""# Context \n\nMotivated by Gregory Smith's web scrape of VGChartz [Video Games Sales][1], this data set simply extends the number of variables with another web scrape from [Metacritic][2]. Unfortunately, there are missing observations as Metacritic only covers a subset of the platforms. Also, a game may not have all the observations of the additional variables discussed below. Complete cases are ~ 6,900\n\n\n# Content\nAlongside the fields: Name, Platform, Year_of_Release, Genre, Publisher, NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales, we have:-\n\n - Critic_score - Aggregate score compiled by Metacritic staff\n - Critic_count - The number of critics used in coming up with the Critic_score\n - User_score - Score by Metacritic's subscribers\n - User_count - Number of users who gave the user_score\n - Developer - Party responsible for creating the game\n - Rating  - The [ESRB][3] ratings\n\n   \n    \n   \n\n# Acknowledgements\nThis repository, https://github.com/wtamu-cisresearch/scraper, after a few adjustments worked extremely well!\n\n\n# Inspiration\nIt would be interesting to see any machine learning techniques or continued data visualizations applied on this data set.\n\n\n  [1]: https://www.kaggle.com/gregorut/datasets\n  [2]: http://www.metacritic.com/browse/games/release-date/available\n  [3]: https://www.esrb.org/""","b""['video games', 'small', 'featured']""",https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings
b'Historical American Lynching',b'Information on 2806 lynchings in the United States',"b'### Context: \n\n""Lynching"" historically includes not only Southern lynching but frontier lynching and vigilantism nationwide and many labor-related incidents.  Persons of any race or ethnicity and either gender may have been either perpetrators or victims of lynching. The lynchings in this dataset follow an NAACP definition for including an incident in the inventory of lynchings:\n \n\n 1.  There must be evidence that someone was killed;\n 2. The killing must have occurred illegally;\n 3. Three or more persons must have taken part in the killing; and\n 4. The killers must have claimed to be serving justice or tradition.\n\n### Content: \n\nThe original data came from the NAACP Lynching Records at Tuskegee Institute, Tuskegee, Alabama.  Stewart Tolnay and E.M. Beck examined these records for name and event duplications and other errors with funding from a National Science Foundation Grant and made their findings available to Project HAL in 1998. Project HAL is inactive now, but it\xe2\x80\x99s original purpose was to build a data set for researchers to use and to add to. \n\nThe dataset contains the following information for each of the 2806 reported lynchings:\n\n* State: State where the lynching took place\n* Year: Year of the lynching\n* Mo: Month\n* Day: Day\n* Victim: Name of the victim\n* County: County where the lynching occurred (keep in mind that county names have changed & boundaries redrawn)\n* Race: Race of the victim\n* Sex: Sex of the victim\n* Mob: Information on the mob\n* Offense: Victim\xe2\x80\x99s alleged offense\n* Note: Note (if any)\n* 2nd Name: Name of the 2nd victim (if any)\n* 3rd Name: Name of the 3rd victim (if any)\n* Comments: Comments (if any)\n* Source: Source of the information (if any)\n\n### Acknowledgements: \n\nThis dataset was compiled by Dr. Elizabeth Hines and Dr. Eliza Steelwater. If you use this dataset in your work, please include the following citation: \n\nHines, E., & Steelwater, E. (2006). Project Hal: Historical American Lynching Data Collection Project. University of North Carolina, http://people.uncw.edu/hinese/HAL/HAL%20Web%20Page.htm\n\n### You may also like:\n\n* Bryan Stevenson\xe2\x80\x99s Equal Justice Initiative, EJI posts lynching information and stories and is currently quite active:  https://www.eji.org/\n* [First Person Narratives of the American South: Personal accounts of Southern life between 1860 and 1920](https://www.kaggle.com/docsouth-data/first-person-narratives-of-the-american-south)\n\n### Inspiration: \n\n* Can you use the county-level data in this dataset to create a map of lynchings in the US?\n* What demographic qualities were most associated with lynching victims?\n* How did patterns of lynching change over time?'","b""['crime', 'united states', 'violence', 'death', 'small', 'featured']""",https://www.kaggle.com/rtatman/historical-american-lynching
"b'State Energy System Data, 1960-2014'",b'State Energy Data Systems (SEDS) data for all US states including DC',"b'#State Energy Data Systems (SEDS) data for all US states, including DC, from 1960 to 2014F\n\n# Context \n\nThis dataset is derived from my general interest in energy systems. It was originally composed for [this exercise](https://data-science-projects.github.io/plotly_examples/seds/seds.html), as part of this [Coursera/John Hopkins Data Science Specilisation](https://www.coursera.org/specializations/jhu-data-science).\n\nThe code that produced this dataset is in https://www.kaggle.com/nathanto/d/nathanto/seds-1960-2014F/data-wrangling-code-for-seds-1960-2014f\n\n# Content\n\nThe data is a composition of the State Energy Data Systems (SEDS) data for all US states, including DC, from 2016 to 2014F, for data released June 29, 2016. It has been tidied from a wide format to a long format, and includes unit codes for the values associated with the observations for each MSN code for each state for each year. \n\nThe ""F"" in the final year number indicates that these are the final observations. There is a lag of some 18 months after year end and final readings.\n\nThe columns are:\n\n - state - State postal code, composed from the function `states.abb` and including ""DC"".\n - msn - A mnemonic series name identifying the value being observed.\n - year - Year of the observation.\n - value - Of the observation.\n - units_code, representing the units of the value, e.g. BBtu is Billion British Thermal Units.\n \nNote that the units_codes are mostly my own invention, based on the [EIA Writng Style Guide](https://www.eia.gov/about/styleguide2015.pdf).\n\n# Acknowledgements\n\nThank you to the [US Energy Information Administration](https://www.eia.gov) for making the data available.\n\nSpecial thanks to Yvonne Taylor for guidance on style for the codes.\n\n# Inspiration\n\nThe first goal for this data was to support some plotting and forecast testing exercises, which is a work in progress. To what extent do past observations predict future observations? Since the data is readily available, and consistent, within limits, over a long period, this format is a good basis for experimenting with techniques in that space. \n\n'","b""['energy', 'medium', 'featured']""",https://www.kaggle.com/nathanto/seds-1960-2014F
b'Moscow Ring Roads',b'Shapefiles for use in Sberbank',"b'# Context \n\nThis data set was created for use in the [Sberbank][1] Kaggle competition. \n\n# Content\n\nThe data consists of three GIS shapefiles one for each of the 3 major Moscow ring roads; the MKAD, TTK (or third ring) and Sadovoe (or garden ring).\n\n# Acknowledgements\n\nThe road shapefiles have been extracted from OpenStreetMap data, processed in QGiS to extract only the roads of interest.   \n\nOpenStreetMap License: https://www.OpenStreetMap.org/copyright \n\n# Inspiration\n\nWith these files and the distances given in the Sberbank dataset it should be possible to better understand the location of properties. With a better understanding of location it may be possible to improve the quality of the overall dataset which contains material amounts of missing or poorly coded data.\n\n\n  [1]: https://www.kaggle.com/c/sberbank-russian-housing-market'","b""['geography', 'russia', 'small', 'featured']""",https://www.kaggle.com/nigelcarpenter/sberbankmoscowroads
b'Fact-Checking Facebook Politics Pages',b'Hyperpartisan Facebook pages and misleading information during the 2016 election',"b""### Context\n\nDuring the 2016 US presidential election, the phrase \xe2\x80\x9cfake news\xe2\x80\x9d found its way to the forefront in news articles, tweets, and fiery online debates the world over after misleading and untrue stories proliferated rapidly. [BuzzFeed News][1] analyzed over 1,000 stories from hyperpartisan political Facebook pages selected from the right, left, and mainstream media to determine the nature and popularity of false or misleading information they shared.\n \n### Content\n\nThis dataset supports the original story [\xe2\x80\x9cHyperpartisan Facebook Pages Are Publishing False And Misleading Information At An Alarming Rate\xe2\x80\x9d][2] published October 20th, 2016. Here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story):\n  \n**More on Our Methodology and Data Limitations**\n\n\xe2\x80\x9cEach of our raters was given a rotating selection of pages from each category on different days. In some cases, we found that pages would repost the same link or video within 24 hours, which caused Facebook to assign it the same URL. When this occurred, we did not log or rate the repeat post and instead kept the original date and rating. Each rater was given the same guide for how to review posts:\n \n* \xe2\x80\x9c*Mostly True*: The post and any related link or image are based on factual information and portray it accurately. This lets them interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up. This rating does not allow for unsupported speculation or claims.\n \n* \xe2\x80\x9c*Mixture of True and False*: Some elements of the information are factually accurate, but some elements or claims are not. This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. Finally, use this rating for news articles that are based on unconfirmed information.    \n\n* \xe2\x80\x9c*Mostly False*: Most or all of the information in the post or in the link being shared is inaccurate. This should also be used when the central claim being made is false.    \n \n* \xe2\x80\x9c*No Factual Content*: This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim. This is also the category to use for posts that are of the \xe2\x80\x9cLike this if you think...\xe2\x80\x9d variety.\n \n\xe2\x80\x9cIn gathering the Facebook engagement data, the API did not return results for some posts. It did not return reaction count data for two posts, and two posts also did not return comment count data. There were 70 posts for which the API did not return share count data. We also used CrowdTangle's API to check that we had entered all posts from all nine pages on the assigned days. In some cases, the API returned URLs that were no longer active. We were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the URLs were returned in error.\xe2\x80\x9d\n \n### Acknowledgements\n \nThis dataset was originally published on GitHub by BuzzFeed News here: https://github.com/BuzzFeedNews/2016-10-facebook-fact-check \n \n### Inspiration\n \nHere are some ideas for exploring the hyperpartisan echo chambers on Facebook:\n \n* How do left, mainstream, and right categories of Facebook pages differ in the stories they share?\n\n* Which types of stories receive the most engagement from their Facebook followers? Are videos or links more effective for engagement?\n\n* Can you replicate BuzzFeed\xe2\x80\x99s findings that \xe2\x80\x9cthe least accurate pages generated some of the highest numbers of shares, reactions, and comments on Facebook\xe2\x80\x9d?\n\n\n#[Start a new kernel][3]\n\n\n  [1]: https://www.kaggle.com/buzzfeed\n  [2]: https://www.buzzfeed.com/craigsilverman/partisan-fb-pages-analysis?utm_term=.kq9kqJDZ2#.ia1QB2KJl.\n  [3]: https://www.kaggle.com/buzzfeed/fact-checking-facebook-politics-pages/kernels?modal=true""","b""['internet', 'politics', 'news agencies', 'political science', 'small', 'featured']""",https://www.kaggle.com/mrisdal/fact-checking-facebook-politics-pages
b'JACS Papers 1996 - 2016',b'All papers published in J. Am. Chem. Soc. between 1996 and 2016',"b'### Context \n\nThe Journal of the American Chemical Society is the premier journal published by the American Chemical Society and one of the highest ranking journals in all of chemistry. With almost 60,000 papers and over 120,000 authors this collections of papers published between 1996 and 2016, represents the current state of chemistry research.\n\n\n### Content\n\nThis dataset is presented in 3 database tables, one of published articles, and one of all authors, with a further table relating authors to the journal articles they have published. \n\nUpdate 21-12-2017: The previous data was collected by a top level scrape of the table of contents pages from the journal.\n\nA couple of months ago I performed a page-level scrape and then forgot about it, but I had some positive reactions to the data-set this week, so I have processed some of the data (although more remains, the raw output from the scrape is an 18 GB csv file).\n\nThis new data updates the Articles table to contain many more data fields, including the paper abstract, number of citations and page views (page views are a relatively new feature, so is probably not for the lifetime of some of the older papers).\n\nOne interesting project for this data would be to look at the term frequencies in the abstracts of the papers, and use that to see how the focus of chemistry research has changed over the years.\n\nIf it is interesting to people, I have the following unprocessed data: - Articles citing papers in this database inc, Title, Journal, Year and Author list - Institutions of authors for each papers (this data is very complicated and requires some difficult parsing)\n\n### Acknowledgements\n\nThis data was scraped from the Table of Contents section of the JACS website and is available online publicly.\n\n### Inspiration\n\nThis data could be used to determine the average number of authors per paper, or the connections between authors, to determine if specific research fields can be grouped by the associated authors\n\nAlso see if you can find the two papers published by me in this year, and see who my co-authors were!'","b""['research', 'chemistry', 'medium', 'featured']""",https://www.kaggle.com/mathewsavage/jacs
b'Match Statistics from top 5 European Leagues',"b'Italy, Spain, England, Germany, France 2012-2017'","b'# Context \nI am a student exploring the possibility of making money in football betting. I am currently doing a literature review on modelling association football scores and trying to put together a machine learning system to use for my first betting campaign next season. What I have learned thus far is that outcomes of football events are partly deterministic and partly random. I do not know exactly how to go about implementing this in a machine learning system yet. I am also hoping to find useful features from this dataset.\n\n\n# Content\nThe data here contains match statistics collected from whoscored.com europes top five leagues from 2012-2013 to 2016-2017 season. It contains just about all match statistics that anyone can ever hope for including but not limited to \nGoals, Corners, Possession, Ratings, Coaches, LineUps, and other relevent match statistics\n\nThe features are simply just self explanatory and have been given long but meaningful names\n\n#Acknowledgement\nI collected the data from the whoscored.com website. I scraped it using beautifulSoup in python and just extracted the features I thought could have some use.\n\n# Inspiration\nThis is just something I hope could become something but hey, it may be nothing. I am just interested to know the kind of insights that could be generated from this.'","b""['association football', 'small', 'featured']""",https://www.kaggle.com/jangot/ligue1-match-statistics
